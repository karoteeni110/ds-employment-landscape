{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d559fd-f448-4aba-a950-667d96ab7e7f",
   "metadata": {},
   "source": [
    "# Recrawl the job description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e87bbc-daf3-4d65-80fd-3410bbd5c285",
   "metadata": {},
   "source": [
    "## #1 Fix the crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1289c7-a1bb-410d-8c2e-6563ebf62094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "\n",
    "        job_description_elem = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\")\n",
    "        inner_html = job_description_elem.get_attribute(name=\"innerHTML\")\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "        \n",
    "        self.job_description = '\\n'.join(\n",
    "            elem.get_text() for elem in soup.find_all() if elem.name in [\"span\", \"ul\", \"strong\", \"li\"]\n",
    "        )\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ccde50-9734-4860-9fe5-5ff74cad2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from linkedin_scraper import actions\n",
    "\n",
    "import os\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "    \n",
    "def crawl_jd(driver, job_link):\n",
    "    job = _Job(linkedin_url=job_link, driver=driver, close_on_complete=False, scrape=True)\n",
    "    return job.job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc73ba4-bc08-4a7b-a27a-0fa4c2b67355",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e95321f-dc37-4de6-b098-5de6926d4ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae228ea-5447-4e42-a980-ca039f70545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-28.csv\n",
      "...130 of 383 jobs done.\n",
      "...140 of 383 jobs done.\n",
      "...150 of 383 jobs done.\n",
      "...160 of 383 jobs done.\n",
      "...170 of 383 jobs done.\n",
      "...180 of 383 jobs done.\n",
      "...190 of 383 jobs done.\n",
      "...200 of 383 jobs done.\n",
      "...210 of 383 jobs done.\n",
      "...220 of 383 jobs done.\n",
      "...230 of 383 jobs done.\n",
      "...240 of 383 jobs done.\n",
      "...250 of 383 jobs done.\n",
      "...260 of 383 jobs done.\n",
      "...270 of 383 jobs done.\n",
      "...280 of 383 jobs done.\n",
      "...290 of 383 jobs done.\n",
      "...300 of 383 jobs done.\n",
      "...310 of 383 jobs done.\n",
      "...320 of 383 jobs done.\n",
      "...330 of 383 jobs done.\n",
      "...340 of 383 jobs done.\n",
      "...350 of 383 jobs done.\n",
      "...360 of 383 jobs done.\n",
      "...370 of 383 jobs done.\n",
      "...380 of 383 jobs done.\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-29.csv\n",
      "...100 of 373 jobs done.\n",
      "...110 of 373 jobs done.\n",
      "...120 of 373 jobs done.\n",
      "...130 of 373 jobs done.\n",
      "...140 of 373 jobs done.\n",
      "...150 of 373 jobs done.\n",
      "...160 of 373 jobs done.\n",
      "...170 of 373 jobs done.\n",
      "...180 of 373 jobs done.\n",
      "...190 of 373 jobs done.\n",
      "...200 of 373 jobs done.\n",
      "...210 of 373 jobs done.\n",
      "...220 of 373 jobs done.\n",
      "...230 of 373 jobs done.\n",
      "...240 of 373 jobs done.\n",
      "...250 of 373 jobs done.\n",
      "...260 of 373 jobs done.\n",
      "...270 of 373 jobs done.\n",
      "...280 of 373 jobs done.\n",
      "...290 of 373 jobs done.\n",
      "...300 of 373 jobs done.\n",
      "...310 of 373 jobs done.\n",
      "...320 of 373 jobs done.\n",
      "...330 of 373 jobs done.\n",
      "...340 of 373 jobs done.\n",
      "...350 of 373 jobs done.\n",
      "...360 of 373 jobs done.\n",
      "...370 of 373 jobs done.\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-30.csv\n",
      "...10 of 361 jobs done.\n",
      "...20 of 361 jobs done.\n",
      "...30 of 361 jobs done.\n",
      "...40 of 361 jobs done.\n",
      "...50 of 361 jobs done.\n",
      "...60 of 361 jobs done.\n",
      "...70 of 361 jobs done.\n",
      "...80 of 361 jobs done.\n",
      "...90 of 361 jobs done.\n",
      "...100 of 361 jobs done.\n",
      "...110 of 361 jobs done.\n",
      "...120 of 361 jobs done.\n",
      "...130 of 361 jobs done.\n",
      "...140 of 361 jobs done.\n",
      "...150 of 361 jobs done.\n",
      "...160 of 361 jobs done.\n",
      "...170 of 361 jobs done.\n",
      "...180 of 361 jobs done.\n",
      "...190 of 361 jobs done.\n",
      "...200 of 361 jobs done.\n",
      "...210 of 361 jobs done.\n",
      "...220 of 361 jobs done.\n",
      "...230 of 361 jobs done.\n",
      "...240 of 361 jobs done.\n",
      "...250 of 361 jobs done.\n",
      "...260 of 361 jobs done.\n",
      "...270 of 361 jobs done.\n",
      "...280 of 361 jobs done.\n",
      "...290 of 361 jobs done.\n",
      "...300 of 361 jobs done.\n",
      "...310 of 361 jobs done.\n",
      "...320 of 361 jobs done.\n",
      "...330 of 361 jobs done.\n",
      "...340 of 361 jobs done.\n",
      "...350 of 361 jobs done.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from utils import standardize_job_urls\n",
    "\n",
    "data_dir_path = Path(os.path.abspath(\"..\"), \"data\")\n",
    "\n",
    "start_date = datetime(2023, 10, 28)\n",
    "end_date = datetime(2023, 11, 4)\n",
    "\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "    csv_path = Path(data_dir_path, f\"crawled_jobs_{date.strftime('%Y-%m-%d')}.csv\")\n",
    "    print(csv_path)\n",
    "    \n",
    "    new_daily_data = pd.read_csv(csv_path)\n",
    "    new_daily_data = new_daily_data.pipe(standardize_job_urls)\n",
    "    # new_daily_data.loc[:,\"job_description\"] = new_daily_data.linkedin_url.map(\n",
    "    #     lambda job_link: crawl_jd(driver, job_link)\n",
    "    # )\n",
    "    if \"jd_recrawled\" not in new_daily_data.columns:\n",
    "        new_daily_data.loc[:,\"jd_recrawled\"] = False\n",
    "    for i, row in new_daily_data.iterrows():\n",
    "        if row.jd_recrawled:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            jd = crawl_jd(driver, row.linkedin_url)\n",
    "            new_daily_data.loc[i,\"job_description\"] = jd\n",
    "            new_daily_data.loc[i,\"jd_recrawled\"] = True\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        if (i+1)%10==0:\n",
    "            print(f\"...{i+1} of {len(new_daily_data)} jobs done.\")\n",
    "            new_daily_data.to_csv(csv_path, index=False)\n",
    "        \n",
    "    new_daily_data.to_csv(csv_path, index=False)\n",
    "    print(\"... CSV re-crawling done.\")\n",
    "    print()\n",
    "\n",
    "    date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c68d69-812d-4846-bcce-e61c4759d0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a1edd-7adb-4bbf-a672-fbfb84d14e32",
   "metadata": {},
   "source": [
    "# Scrape Linkedin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d7fc9e-da88-4ef3-9af4-85ef9ecd6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper==2.11.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have installed the dependency\n",
    "! pip freeze | grep linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50695a9e-6b21-44db-825b-922a213bbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Chrome 114.0.5735.90 \n"
     ]
    }
   ],
   "source": [
    "! google-chrome-stable --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755f7ef-09a8-4caa-809f-551ba6949eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, Job, actions\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "\n",
    "class _JobSearch(JobSearch):\n",
    "    def __init__(self, final_url=None, **kwargs):\n",
    "        self.final_url = final_url\n",
    "        self.current_url = None\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def search(self, search_term: str, page_n) -> List[Job]:\n",
    "        if self.final_url is None:\n",
    "            self.current_url = os.path.join(self.base_url, \"search\") + f\"?keywords={urllib.parse.quote(search_term)}&refresh=true\"\n",
    "            self.driver.get(self.current_url)\n",
    "\n",
    "            # Get redirection URL\n",
    "            self.final_url = self.driver.current_url\n",
    "        else:\n",
    "            self.current_url = os.path.join(self.final_url, f\"&start={25*(page_n-1)}\")\n",
    "            self.driver.get(self.current_url)\n",
    "        \n",
    "        self.scroll_to_bottom()\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_listing_class_name = \"jobs-search-results-list\"\n",
    "        job_listing = self.wait_for_element_to_load(name=job_listing_class_name)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.3)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.6)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 1)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_results = []\n",
    "        for job_card in self.wait_for_all_elements_to_load(name=\"job-card-list\", base=job_listing):\n",
    "            job = self.scrape_job_card(job_card)\n",
    "            job_results.append(job)\n",
    "        return job_results\n",
    "\n",
    "def are_same(job1: Job, job2: Job):\n",
    "    if job1.job_title == job2.job_title and job1.company == job2.company:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612fd0-f5c4-4b3a-b2ee-578b132f471c",
   "metadata": {},
   "source": [
    "## 1. Scrape Job Search\n",
    "\n",
    "Scrape the first 50 pages of the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771e66e2-2364-4cdb-b4f1-f2abb4ec7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up the lower-level services for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\")\n",
    "job_search = _JobSearch(driver=driver, close_on_complete=False, scrape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d87d07e-659d-4abb-a058-e6d2c14e108e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Searching jobs... Keyword: data; Page 1/50'\n",
      "'FINISHED PAGE: 1'\n",
      "'Searching jobs... Keyword: data; Page 2/50'\n",
      "'FINISHED PAGE: 2'\n",
      "'Searching jobs... Keyword: data; Page 3/50'\n",
      "'FINISHED PAGE: 3'\n",
      "'Searching jobs... Keyword: data; Page 4/50'\n",
      "'FINISHED PAGE: 4'\n",
      "'Searching jobs... Keyword: data; Page 5/50'\n",
      "'FINISHED PAGE: 5'\n",
      "'Searching jobs... Keyword: data; Page 6/50'\n",
      "'FINISHED PAGE: 6'\n",
      "'Searching jobs... Keyword: data; Page 7/50'\n",
      "'FINISHED PAGE: 7'\n",
      "'Searching jobs... Keyword: data; Page 8/50'\n",
      "'FINISHED PAGE: 8'\n",
      "'Searching jobs... Keyword: data; Page 9/50'\n",
      "'FINISHED PAGE: 9'\n",
      "'Searching jobs... Keyword: data; Page 10/50'\n",
      "'FINISHED PAGE: 10'\n",
      "'Searching jobs... Keyword: data; Page 11/50'\n",
      "'FINISHED PAGE: 11'\n",
      "'Searching jobs... Keyword: data; Page 12/50'\n",
      "'FINISHED PAGE: 12'\n",
      "'Searching jobs... Keyword: data; Page 13/50'\n",
      "'FINISHED PAGE: 13'\n",
      "'Searching jobs... Keyword: data; Page 14/50'\n",
      "'FINISHED PAGE: 14'\n",
      "'Searching jobs... Keyword: data; Page 15/50'\n",
      "'FINISHED PAGE: 15'\n",
      "'Searching jobs... Keyword: data; Page 16/50'\n",
      "'FINISHED PAGE: 16'\n",
      "'Searching jobs... Keyword: data; Page 17/50'\n",
      "'FINISHED PAGE: 17'\n",
      "'Searching jobs... Keyword: data; Page 18/50'\n",
      "'FINISHED PAGE: 18'\n",
      "'Searching jobs... Keyword: data; Page 19/50'\n",
      "'FINISHED PAGE: 19'\n",
      "'Searching jobs... Keyword: data; Page 20/50'\n",
      "'FINISHED PAGE: 20'\n",
      "'Searching jobs... Keyword: data; Page 21/50'\n",
      "'FINISHED PAGE: 21'\n",
      "'Searching jobs... Keyword: data; Page 22/50'\n",
      "'SKIPPED PAGE: 22'\n",
      "'Searching jobs... Keyword: data; Page 23/50'\n",
      "'SKIPPED PAGE: 23'\n",
      "'Searching jobs... Keyword: data; Page 24/50'\n",
      "'SKIPPED PAGE: 24'\n",
      "'Searching jobs... Keyword: data; Page 25/50'\n",
      "'SKIPPED PAGE: 25'\n",
      "'Searching jobs... Keyword: data; Page 26/50'\n",
      "'SKIPPED PAGE: 26'\n",
      "'Searching jobs... Keyword: data; Page 27/50'\n",
      "'SKIPPED PAGE: 27'\n",
      "'Searching jobs... Keyword: data; Page 28/50'\n",
      "'SKIPPED PAGE: 28'\n",
      "'Searching jobs... Keyword: data; Page 29/50'\n",
      "'SKIPPED PAGE: 29'\n",
      "'Searching jobs... Keyword: data; Page 30/50'\n",
      "'SKIPPED PAGE: 30'\n",
      "'Searching jobs... Keyword: data; Page 31/50'\n",
      "'SKIPPED PAGE: 31'\n",
      "'Searching jobs... Keyword: data; Page 32/50'\n",
      "'SKIPPED PAGE: 32'\n",
      "'Searching jobs... Keyword: data; Page 33/50'\n",
      "'SKIPPED PAGE: 33'\n",
      "'Searching jobs... Keyword: data; Page 34/50'\n",
      "'SKIPPED PAGE: 34'\n",
      "'Searching jobs... Keyword: data; Page 35/50'\n",
      "'SKIPPED PAGE: 35'\n",
      "'Searching jobs... Keyword: data; Page 36/50'\n",
      "'SKIPPED PAGE: 36'\n",
      "'Searching jobs... Keyword: data; Page 37/50'\n",
      "'SKIPPED PAGE: 37'\n",
      "'Searching jobs... Keyword: data; Page 38/50'\n",
      "'SKIPPED PAGE: 38'\n",
      "'Searching jobs... Keyword: data; Page 39/50'\n",
      "'SKIPPED PAGE: 39'\n",
      "'Searching jobs... Keyword: data; Page 40/50'\n",
      "'SKIPPED PAGE: 40'\n",
      "'Searching jobs... Keyword: data; Page 41/50'\n",
      "'SKIPPED PAGE: 41'\n",
      "'Searching jobs... Keyword: data; Page 42/50'\n",
      "'FINISHED PAGE: 42'\n",
      "'Searching jobs... Keyword: data; Page 43/50'\n",
      "'FINISHED PAGE: 43'\n",
      "'Searching jobs... Keyword: data; Page 44/50'\n",
      "'FINISHED PAGE: 44'\n",
      "'Searching jobs... Keyword: data; Page 45/50'\n",
      "'FINISHED PAGE: 45'\n",
      "'Searching jobs... Keyword: data; Page 46/50'\n",
      "'FINISHED PAGE: 46'\n",
      "'Searching jobs... Keyword: data; Page 47/50'\n",
      "'FINISHED PAGE: 47'\n",
      "'Searching jobs... Keyword: data; Page 48/50'\n",
      "'FINISHED PAGE: 48'\n",
      "'Searching jobs... Keyword: data; Page 49/50'\n",
      "'FINISHED PAGE: 49'\n",
      "'Searching jobs... Keyword: data; Page 50/50'\n",
      "'FINISHED PAGE: 50'\n",
      "CPU times: user 3.88 s, sys: 763 ms, total: 4.65 s\n",
      "Wall time: 25min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "N_PAGES = 50\n",
    "SEARCH_KEYWORD = \"data\"\n",
    "\n",
    "jobs = []\n",
    "for page_n in range(1, N_PAGES+1):\n",
    "    pprint(f\"Searching jobs... Keyword: {SEARCH_KEYWORD}; Page {page_n}/{N_PAGES}\")\n",
    "    try:\n",
    "        new_batch = job_search.search(SEARCH_KEYWORD, page_n)\n",
    "    except TimeoutException:\n",
    "        pprint(f\"SKIPPED PAGE: {page_n}\")\n",
    "        continue\n",
    "\n",
    "    # Check if the new batch of jobs are duplicates, \n",
    "    # which means we have gone through all the pages and should quit scraping.\n",
    "    if jobs and are_same(new_batch[0], jobs[0]):\n",
    "        pprint(\"Found duplicate results! All the pages have been scraped. Quiting...\")\n",
    "        break\n",
    "        \n",
    "    jobs.extend(new_batch)\n",
    "    pprint(f\"FINISHED PAGE: {page_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ca956d-a4cb-443d-8d9d-7bbc96e6e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500ab9ca-d346-4b90-abb0-fafdffefca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl temporarily\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "with open(f\"../data/tmp/{fname}\", \"wb\") as f:\n",
    "    dicted_jobs = [job.to_dict() for job in jobs]\n",
    "    pickle.dump(dicted_jobs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f02c2-cd94-4e16-bb3e-3c3c4b761913",
   "metadata": {},
   "source": [
    "## 2. Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17c1166-5c4d-4ee4-a684-10c0535f32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job, actions\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "        self.job_title = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-unified-top-card__job-title')]\").text.strip()\n",
    "        self.company = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a[1]\").text.strip()\n",
    "        self.company_linkedin_url = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a\").get_attribute(\"href\")\n",
    "        self.location = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//*\").text.strip()\n",
    "        self.posted_date = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//span[3]\").text.strip()\n",
    "        self.job_type_1 = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')]/span\").text.strip()\n",
    "        self.job_description = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\").text.strip()\n",
    "        \n",
    "        try:\n",
    "            self.required_skills = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][1]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.required_skills += self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][2]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.job_type_2 = self.wait_for_element_to_load(by=By.XPATH, name=\"(//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')])[2]/span\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.job_type_2 = \"\"\n",
    "            \n",
    "        try:\n",
    "            self.applicant_count = self.wait_for_element_to_load(by=By.XPATH, name=\"jobs-unified-top-card__applicant-count\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.applicant_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.benefits = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'salary-main-rail-card')]\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.benefits = \"\"\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0690bf7-4c55-434f-a72b-fae89fd21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b010a69b-bf8d-434e-ba07-641b09ff211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up low-level servies for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5236-f815-4a94-b6b4-f0b368132f1e",
   "metadata": {},
   "source": [
    "Ignore the error logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b8e6e6-ed7c-4f70-8f48-8e5b463c681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "\n",
    "with open(f\"../data/tmp/{fname}\", \"rb\") as f:\n",
    "    jobs = pickle.load(f)\n",
    "\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10b00c9-d1d6-4723-89a1-7823fd0f4c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 1/381\n",
      "Crawling... Jobs 2/381\n",
      "Crawling... Jobs 3/381\n",
      "Crawling... Jobs 4/381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55d8897ea4e3 <unknown>\n",
      "#1 0x55d889519c76 <unknown>\n",
      "#2 0x55d889555c96 <unknown>\n",
      "#3 0x55d889555dc1 <unknown>\n",
      "#4 0x55d88958f7f4 <unknown>\n",
      "#5 0x55d88957503d <unknown>\n",
      "#6 0x55d88958d30e <unknown>\n",
      "#7 0x55d889574de3 <unknown>\n",
      "#8 0x55d88954a2dd <unknown>\n",
      "#9 0x55d88954b34e <unknown>\n",
      "#10 0x55d8897aa3e4 <unknown>\n",
      "#11 0x55d8897ae3d7 <unknown>\n",
      "#12 0x55d8897b8b20 <unknown>\n",
      "#13 0x55d8897af023 <unknown>\n",
      "#14 0x55d88977d1aa <unknown>\n",
      "#15 0x55d8897d36b8 <unknown>\n",
      "#16 0x55d8897d3847 <unknown>\n",
      "#17 0x55d8897e3243 <unknown>\n",
      "#18 0x7f08c1a94ac3 <unknown>\n",
      "\n",
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55d8897ea4e3 <unknown>\n",
      "#1 0x55d889519c76 <unknown>\n",
      "#2 0x55d889555c96 <unknown>\n",
      "#3 0x55d889555dc1 <unknown>\n",
      "#4 0x55d88958f7f4 <unknown>\n",
      "#5 0x55d88957503d <unknown>\n",
      "#6 0x55d88958d30e <unknown>\n",
      "#7 0x55d889574de3 <unknown>\n",
      "#8 0x55d88954a2dd <unknown>\n",
      "#9 0x55d88954b34e <unknown>\n",
      "#10 0x55d8897aa3e4 <unknown>\n",
      "#11 0x55d8897ae3d7 <unknown>\n",
      "#12 0x55d8897b8b20 <unknown>\n",
      "#13 0x55d8897af023 <unknown>\n",
      "#14 0x55d88977d1aa <unknown>\n",
      "#15 0x55d8897d36b8 <unknown>\n",
      "#16 0x55d8897d3847 <unknown>\n",
      "#17 0x55d8897e3243 <unknown>\n",
      "#18 0x7f08c1a94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 5/381\n",
      "Crawling... Jobs 6/381\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_CONNECTION_REFUSED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x55d8897ea4e3 <unknown>\n#1 0x55d889519c76 <unknown>\n#2 0x55d889511c7f <unknown>\n#3 0x55d889503ca2 <unknown>\n#4 0x55d889505412 <unknown>\n#5 0x55d8895040ca <unknown>\n#6 0x55d889503168 <unknown>\n#7 0x55d889502fa0 <unknown>\n#8 0x55d8895019bf <unknown>\n#9 0x55d889501fed <unknown>\n#10 0x55d88951bb06 <unknown>\n#11 0x55d88958d9e5 <unknown>\n#12 0x55d889575012 <unknown>\n#13 0x55d88958d30e <unknown>\n#14 0x55d889574de3 <unknown>\n#15 0x55d88954a2dd <unknown>\n#16 0x55d88954b34e <unknown>\n#17 0x55d8897aa3e4 <unknown>\n#18 0x55d8897ae3d7 <unknown>\n#19 0x55d8897b8b20 <unknown>\n#20 0x55d8897af023 <unknown>\n#21 0x55d88977d1aa <unknown>\n#22 0x55d8897d36b8 <unknown>\n#23 0x55d8897d3847 <unknown>\n#24 0x55d8897e3243 <unknown>\n#25 0x7f08c1a94ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36m_Job.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:40\u001b[0m, in \u001b[0;36mJob.__init__\u001b[0;34m(self, linkedin_url, job_title, company, company_linkedin_url, location, posted_date, applicant_count, job_description, benefits, driver, close_on_complete, scrape)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m benefits\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scrape:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:47\u001b[0m, in \u001b[0;36mJob.scrape\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed_in():\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_logged_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis part is not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36m_Job.scrape_logged_in\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_logged_in\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinkedin_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfocus()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_element_to_load(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mXPATH, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//*[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobs-unified-top-card__job-title\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_CONNECTION_REFUSED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x55d8897ea4e3 <unknown>\n#1 0x55d889519c76 <unknown>\n#2 0x55d889511c7f <unknown>\n#3 0x55d889503ca2 <unknown>\n#4 0x55d889505412 <unknown>\n#5 0x55d8895040ca <unknown>\n#6 0x55d889503168 <unknown>\n#7 0x55d889502fa0 <unknown>\n#8 0x55d8895019bf <unknown>\n#9 0x55d889501fed <unknown>\n#10 0x55d88951bb06 <unknown>\n#11 0x55d88958d9e5 <unknown>\n#12 0x55d889575012 <unknown>\n#13 0x55d88958d30e <unknown>\n#14 0x55d889574de3 <unknown>\n#15 0x55d88954a2dd <unknown>\n#16 0x55d88954b34e <unknown>\n#17 0x55d8897aa3e4 <unknown>\n#18 0x55d8897ae3d7 <unknown>\n#19 0x55d8897b8b20 <unknown>\n#20 0x55d8897af023 <unknown>\n#21 0x55d88977d1aa <unknown>\n#22 0x55d8897d36b8 <unknown>\n#23 0x55d8897d3847 <unknown>\n#24 0x55d8897e3243 <unknown>\n#25 0x7f08c1a94ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from time import sleep\n",
    "\n",
    "N_JOBS = len(jobs)\n",
    "\n",
    "crawled_jobs = []\n",
    "for i, job in enumerate(jobs):\n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except StaleElementReferenceException or TimeoutException:\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad0dbf8-cba2-42bc-a496-57345dde2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260db594-e6f7-4ebc-9294-e132e9ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]\n",
    "                              ).drop(columns=[\"driver\"]\n",
    "                              ).drop_duplicates(\"linkedin_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2d9665-a746-4a98-a1f2-7446de35ac9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QARA Specialist / Senior Specialist</td>\n",
       "      <td>EnglishAttention to Detail, Communication, IEC...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3745632980/...</td>\n",
       "      <td>Topcon Healthcare Europe</td>\n",
       "      <td>https://www.linkedin.com/company/topconhealthc...</td>\n",
       "      <td>Topcon Healthcare Europe · Oulu, North Ostrobo...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nDo you want to have a direct im...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Data Analytics and Data WarehousingData Archit...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3725618198/...</td>\n",
       "      <td>Nortal</td>\n",
       "      <td>https://www.linkedin.com/company/nortal/life</td>\n",
       "      <td>Nortal · Helsinki, Uusimaa, Finland Reposted  ...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOverview\\n\\nDo you enjoy being ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technical Business Analyst in FCIIA</td>\n",
       "      <td>Analytical SkillsAgile Project Management, Bus...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3733051402/...</td>\n",
       "      <td>Nordea</td>\n",
       "      <td>https://www.linkedin.com/company/nordea/life</td>\n",
       "      <td>Nordea · Helsinki, Uusimaa, Finland  2 weeks a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob ID: 19860 \\n Do you have a ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td></td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3756229497/...</td>\n",
       "      <td>Baleen Labs Ltd.</td>\n",
       "      <td>https://www.linkedin.com/company/baleen-labs/life</td>\n",
       "      <td>Baleen Labs Ltd. · European Economic Area  5 h...</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nSee more</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Architect, Digital Society</td>\n",
       "      <td>Data Engineering, Extract, Transform, Load (ET...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3755141358/...</td>\n",
       "      <td>CGI</td>\n",
       "      <td>https://www.linkedin.com/company/cgi/life</td>\n",
       "      <td>CGI · Helsinki, Uusimaa, Finland  1 day ago  ·...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nPosition Description\\n\\nVuonna ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job_title  \\\n",
       "0  QARA Specialist / Senior Specialist   \n",
       "1                       Data Architect   \n",
       "2  Technical Business Analyst in FCIIA   \n",
       "3                Junior Data Scientist   \n",
       "4      Data Architect, Digital Society   \n",
       "\n",
       "                                     required_skills job_type_1 job_type_2  \\\n",
       "0  EnglishAttention to Detail, Communication, IEC...     Hybrid  Full-time   \n",
       "1  Data Analytics and Data WarehousingData Archit...     Hybrid  Full-time   \n",
       "2  Analytical SkillsAgile Project Management, Bus...    On-site  Full-time   \n",
       "3                                                        Remote  Full-time   \n",
       "4  Data Engineering, Extract, Transform, Load (ET...  Full-time              \n",
       "\n",
       "                                        linkedin_url  \\\n",
       "0  https://www.linkedin.com/jobs/view/3745632980/...   \n",
       "1  https://www.linkedin.com/jobs/view/3725618198/...   \n",
       "2  https://www.linkedin.com/jobs/view/3733051402/...   \n",
       "3  https://www.linkedin.com/jobs/view/3756229497/...   \n",
       "4  https://www.linkedin.com/jobs/view/3755141358/...   \n",
       "\n",
       "                    company  \\\n",
       "0  Topcon Healthcare Europe   \n",
       "1                    Nortal   \n",
       "2                    Nordea   \n",
       "3          Baleen Labs Ltd.   \n",
       "4                       CGI   \n",
       "\n",
       "                                company_linkedin_url  \\\n",
       "0  https://www.linkedin.com/company/topconhealthc...   \n",
       "1       https://www.linkedin.com/company/nortal/life   \n",
       "2       https://www.linkedin.com/company/nordea/life   \n",
       "3  https://www.linkedin.com/company/baleen-labs/life   \n",
       "4          https://www.linkedin.com/company/cgi/life   \n",
       "\n",
       "                                            location            posted_date  \\\n",
       "0  Topcon Healthcare Europe · Oulu, North Ostrobo...             3 days ago   \n",
       "1  Nortal · Helsinki, Uusimaa, Finland Reposted  ...  Reposted  2 weeks ago   \n",
       "2  Nordea · Helsinki, Uusimaa, Finland  2 weeks a...            2 weeks ago   \n",
       "3  Baleen Labs Ltd. · European Economic Area  5 h...            5 hours ago   \n",
       "4  CGI · Helsinki, Uusimaa, Finland  1 day ago  ·...              1 day ago   \n",
       "\n",
       "   applicant_count                                    job_description benefits  \n",
       "0                0  About the job\\nDo you want to have a direct im...           \n",
       "1                0  About the job\\nOverview\\n\\nDo you enjoy being ...           \n",
       "2                0  About the job\\nJob ID: 19860 \\n Do you have a ...           \n",
       "3                0                            About the job\\nSee more           \n",
       "4                0  About the job\\nPosition Description\\n\\nVuonna ...           "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58dfc6-3ab7-416e-834f-525cb464aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crawled_jobs.to_csv(f\"../data/crawled_jobs_1-{len(crawled_jobs}_checkpoint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568898a5-df54-4afa-85ff-3b858f901e30",
   "metadata": {},
   "source": [
    "### 2.1 Continue from the failed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e352c75e-aaa9-4bd8-ac86-add24d96cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# In case session expiration\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50eff973-e5db-40c0-9a73-24e02e836e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 348/381\n",
      "Crawling... Jobs 349/381\n",
      "Crawling... Jobs 350/381\n",
      "Crawling... Jobs 351/381\n",
      "Crawling... Jobs 352/381\n",
      "Crawling... Jobs 353/381\n",
      "Crawling... Jobs 354/381\n",
      "Crawling... Jobs 355/381\n",
      "Crawling... Jobs 356/381\n",
      "Crawling... Jobs 357/381\n",
      "Crawling... Jobs 358/381\n",
      "Crawling... Jobs 359/381\n",
      "Crawling... Jobs 360/381\n",
      "Crawling... Jobs 361/381\n",
      "Crawling... Jobs 362/381\n",
      "Crawling... Jobs 363/381\n",
      "Crawling... Jobs 364/381\n",
      "Crawling... Jobs 365/381\n",
      "Crawling... Jobs 366/381\n",
      "Crawling... Jobs 367/381\n",
      "Crawling... Jobs 368/381\n",
      "Crawling... Jobs 369/381\n",
      "Crawling... Jobs 370/381\n",
      "Crawling... Jobs 371/381\n",
      "Crawling... Jobs 372/381\n",
      "Crawling... Jobs 373/381\n",
      "Crawling... Jobs 374/381\n",
      "Crawling... Jobs 375/381\n",
      "Crawling... Jobs 376/381\n",
      "Crawling... Jobs 377/381\n",
      "Crawling... Jobs 378/381\n",
      "Crawling... Jobs 379/381\n",
      "Crawling... Jobs 380/381\n",
      "Crawling... Jobs 381/381\n",
      "CPU times: user 1.58 s, sys: 268 ms, total: 1.85 s\n",
      "Wall time: 7min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Continue\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "\n",
    "CONTINUE_FROM = 348\n",
    "\n",
    "for i, job in enumerate(jobs):\n",
    "    if i+1<CONTINUE_FROM:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except StaleElementReferenceException or TimeoutException:\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f3595eb-d170-4e5a-b7d3-d86b8480a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QARA Specialist / Senior Specialist</td>\n",
       "      <td>EnglishAttention to Detail, Communication, IEC...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3745632980/...</td>\n",
       "      <td>Topcon Healthcare Europe</td>\n",
       "      <td>https://www.linkedin.com/company/topconhealthc...</td>\n",
       "      <td>Topcon Healthcare Europe · Oulu, North Ostrobo...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nDo you want to have a direct im...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Data Analytics and Data WarehousingData Archit...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3725618198/...</td>\n",
       "      <td>Nortal</td>\n",
       "      <td>https://www.linkedin.com/company/nortal/life</td>\n",
       "      <td>Nortal · Helsinki, Uusimaa, Finland Reposted  ...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOverview\\n\\nDo you enjoy being ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technical Business Analyst in FCIIA</td>\n",
       "      <td>Analytical SkillsAgile Project Management, Bus...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3733051402/...</td>\n",
       "      <td>Nordea</td>\n",
       "      <td>https://www.linkedin.com/company/nordea/life</td>\n",
       "      <td>Nordea · Helsinki, Uusimaa, Finland  2 weeks a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob ID: 19860 \\n Do you have a ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td></td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3756229497/...</td>\n",
       "      <td>Baleen Labs Ltd.</td>\n",
       "      <td>https://www.linkedin.com/company/baleen-labs/life</td>\n",
       "      <td>Baleen Labs Ltd. · European Economic Area  5 h...</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nSee more</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Architect, Digital Society</td>\n",
       "      <td>Data Engineering, Extract, Transform, Load (ET...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3755141358/...</td>\n",
       "      <td>CGI</td>\n",
       "      <td>https://www.linkedin.com/company/cgi/life</td>\n",
       "      <td>CGI · Helsinki, Uusimaa, Finland  1 day ago  ·...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nPosition Description\\n\\nVuonna ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Senior Data Analyst, Ads</td>\n",
       "      <td>Data Analysis, Python (Programming Language), ...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3702242885/...</td>\n",
       "      <td>Rovio Entertainment Corporation</td>\n",
       "      <td>https://www.linkedin.com/company/rovio/life</td>\n",
       "      <td>Rovio Entertainment Corporation · Helsinki, Uu...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAt Rovio you will get to work w...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Data Warehouse Engineer</td>\n",
       "      <td>Computer Science, Data Warehousing, Databases,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3749884540/...</td>\n",
       "      <td>Schibsted Finland</td>\n",
       "      <td>https://www.linkedin.com/company/schibsted-fin...</td>\n",
       "      <td>Schibsted Finland · Helsinki, Uusimaa, Finland...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nTHE OPPORTUNITY IN A NUTSHELL\\n...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Senior Analytics Engineer</td>\n",
       "      <td>Analytical Skills, Data Analysis, English, and...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3744849073/...</td>\n",
       "      <td>Ageras</td>\n",
       "      <td>https://www.linkedin.com/company/ageras-global...</td>\n",
       "      <td>Ageras · Espoo, Uusimaa, Finland  1 week ago  ...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nWe are looking for a Senior Ana...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Extract, Transform, Load (ETL), Python (Progra...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3740586858/...</td>\n",
       "      <td>Bean Solutions Oy</td>\n",
       "      <td>https://www.linkedin.com/company/bean-solution...</td>\n",
       "      <td>Bean Solutions Oy · Helsinki Metropolitan Area...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nSinä tietovarastoympäristöistä ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Data Analysis, Python (Programming Language), ...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3750109074/...</td>\n",
       "      <td>Musopia</td>\n",
       "      <td>https://www.linkedin.com/company/musopia/life</td>\n",
       "      <td>Musopia · Helsinki Metropolitan Area  3 days a...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nHow about a backstage pass to t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0    QARA Specialist / Senior Specialist   \n",
       "1                         Data Architect   \n",
       "2    Technical Business Analyst in FCIIA   \n",
       "3                  Junior Data Scientist   \n",
       "4        Data Architect, Digital Society   \n",
       "..                                   ...   \n",
       "371             Senior Data Analyst, Ads   \n",
       "372              Data Warehouse Engineer   \n",
       "373            Senior Analytics Engineer   \n",
       "374                        Data Engineer   \n",
       "375                  Senior Data Analyst   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    EnglishAttention to Detail, Communication, IEC...     Hybrid  Full-time   \n",
       "1    Data Analytics and Data WarehousingData Archit...     Hybrid  Full-time   \n",
       "2    Analytical SkillsAgile Project Management, Bus...    On-site  Full-time   \n",
       "3                                                          Remote  Full-time   \n",
       "4    Data Engineering, Extract, Transform, Load (ET...  Full-time              \n",
       "..                                                 ...        ...        ...   \n",
       "371  Data Analysis, Python (Programming Language), ...     Hybrid  Full-time   \n",
       "372  Computer Science, Data Warehousing, Databases,...     Hybrid  Full-time   \n",
       "373  Analytical Skills, Data Analysis, English, and...     Hybrid  Full-time   \n",
       "374  Extract, Transform, Load (ETL), Python (Progra...     Hybrid  Full-time   \n",
       "375  Data Analysis, Python (Programming Language), ...     Hybrid  Full-time   \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3745632980/...   \n",
       "1    https://www.linkedin.com/jobs/view/3725618198/...   \n",
       "2    https://www.linkedin.com/jobs/view/3733051402/...   \n",
       "3    https://www.linkedin.com/jobs/view/3756229497/...   \n",
       "4    https://www.linkedin.com/jobs/view/3755141358/...   \n",
       "..                                                 ...   \n",
       "371  https://www.linkedin.com/jobs/view/3702242885/...   \n",
       "372  https://www.linkedin.com/jobs/view/3749884540/...   \n",
       "373  https://www.linkedin.com/jobs/view/3744849073/...   \n",
       "374  https://www.linkedin.com/jobs/view/3740586858/...   \n",
       "375  https://www.linkedin.com/jobs/view/3750109074/...   \n",
       "\n",
       "                             company  \\\n",
       "0           Topcon Healthcare Europe   \n",
       "1                             Nortal   \n",
       "2                             Nordea   \n",
       "3                   Baleen Labs Ltd.   \n",
       "4                                CGI   \n",
       "..                               ...   \n",
       "371  Rovio Entertainment Corporation   \n",
       "372                Schibsted Finland   \n",
       "373                           Ageras   \n",
       "374                Bean Solutions Oy   \n",
       "375                          Musopia   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0    https://www.linkedin.com/company/topconhealthc...   \n",
       "1         https://www.linkedin.com/company/nortal/life   \n",
       "2         https://www.linkedin.com/company/nordea/life   \n",
       "3    https://www.linkedin.com/company/baleen-labs/life   \n",
       "4            https://www.linkedin.com/company/cgi/life   \n",
       "..                                                 ...   \n",
       "371        https://www.linkedin.com/company/rovio/life   \n",
       "372  https://www.linkedin.com/company/schibsted-fin...   \n",
       "373  https://www.linkedin.com/company/ageras-global...   \n",
       "374  https://www.linkedin.com/company/bean-solution...   \n",
       "375      https://www.linkedin.com/company/musopia/life   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    Topcon Healthcare Europe · Oulu, North Ostrobo...             3 days ago   \n",
       "1    Nortal · Helsinki, Uusimaa, Finland Reposted  ...  Reposted  2 weeks ago   \n",
       "2    Nordea · Helsinki, Uusimaa, Finland  2 weeks a...            2 weeks ago   \n",
       "3    Baleen Labs Ltd. · European Economic Area  5 h...            5 hours ago   \n",
       "4    CGI · Helsinki, Uusimaa, Finland  1 day ago  ·...              1 day ago   \n",
       "..                                                 ...                    ...   \n",
       "371  Rovio Entertainment Corporation · Helsinki, Uu...  Reposted  2 weeks ago   \n",
       "372  Schibsted Finland · Helsinki, Uusimaa, Finland...             3 days ago   \n",
       "373  Ageras · Espoo, Uusimaa, Finland  1 week ago  ...             1 week ago   \n",
       "374  Bean Solutions Oy · Helsinki Metropolitan Area...            2 weeks ago   \n",
       "375  Musopia · Helsinki Metropolitan Area  3 days a...             3 days ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nDo you want to have a direct im...   \n",
       "1                  0  About the job\\nOverview\\n\\nDo you enjoy being ...   \n",
       "2                  0  About the job\\nJob ID: 19860 \\n Do you have a ...   \n",
       "3                  0                            About the job\\nSee more   \n",
       "4                  0  About the job\\nPosition Description\\n\\nVuonna ...   \n",
       "..               ...                                                ...   \n",
       "371                0  About the job\\nAt Rovio you will get to work w...   \n",
       "372                0  About the job\\nTHE OPPORTUNITY IN A NUTSHELL\\n...   \n",
       "373                0  About the job\\nWe are looking for a Senior Ana...   \n",
       "374                0  About the job\\nSinä tietovarastoympäristöistä ...   \n",
       "375                0  About the job\\nHow about a backstage pass to t...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "371           \n",
       "372           \n",
       "373           \n",
       "374           \n",
       "375           \n",
       "\n",
       "[376 rows x 12 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]).drop(columns=[\"driver\"]).drop_duplicates(\"linkedin_url\")\n",
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bf2bda0-7b3c-425d-87a4-0420c1e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"../data/crawled_jobs_{current_date}.csv\"\n",
    "\n",
    "df_crawled_jobs.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0d8b-2cb2-4217-ab99-6ece5a995d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94094-a7ba-4dd8-96d6-17b734384714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

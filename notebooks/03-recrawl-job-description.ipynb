{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d559fd-f448-4aba-a950-667d96ab7e7f",
   "metadata": {},
   "source": [
    "# Recrawl the job description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e87bbc-daf3-4d65-80fd-3410bbd5c285",
   "metadata": {},
   "source": [
    "## #1 Fix the crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1289c7-a1bb-410d-8c2e-6563ebf62094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "\n",
    "        job_description_elem = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\")\n",
    "        inner_html = job_description_elem.get_attribute(name=\"innerHTML\")\n",
    "        soup = BeautifulSoup(inner_html, 'html.parser')\n",
    "        \n",
    "        self.job_description = '\\n'.join(\n",
    "            elem.get_text() for elem in soup.find_all() if elem.name in [\"span\", \"ul\", \"strong\", \"li\"]\n",
    "        )\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ccde50-9734-4860-9fe5-5ff74cad2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from linkedin_scraper import actions\n",
    "\n",
    "import os\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "    \n",
    "def crawl_jd(driver, job_link):\n",
    "    job = _Job(linkedin_url=job_link, driver=driver, close_on_complete=False, scrape=True)\n",
    "    return job.job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc73ba4-bc08-4a7b-a27a-0fa4c2b67355",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e95321f-dc37-4de6-b098-5de6926d4ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae228ea-5447-4e42-a980-ca039f70545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-28.csv\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-29.csv\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-30.csv\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-10-31.csv\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-11-01.csv\n",
      "...130 of 333 jobs done.\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-11-02.csv\n",
      "...140 of 376 jobs done.\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-11-03.csv\n",
      "... CSV re-crawling done.\n",
      "\n",
      "/home/parallels/repos/ds-employment-landscape/data/crawled_jobs_2023-11-04.csv\n",
      "...310 of 392 jobs done.\n",
      "...320 of 392 jobs done.\n",
      "...330 of 392 jobs done.\n",
      "...340 of 392 jobs done.\n",
      "...350 of 392 jobs done.\n",
      "...360 of 392 jobs done.\n",
      "...370 of 392 jobs done.\n",
      "...380 of 392 jobs done.\n",
      "...390 of 392 jobs done.\n",
      "... CSV re-crawling done.\n",
      "\n",
      "CPU times: user 2.95 s, sys: 808 ms, total: 3.75 s\n",
      "Wall time: 10min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from utils import standardize_job_urls\n",
    "\n",
    "data_dir_path = Path(os.path.abspath(\"..\"), \"data\")\n",
    "\n",
    "start_date = datetime(2023, 10, 28)\n",
    "end_date = datetime(2023, 11, 4)\n",
    "\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "    csv_path = Path(data_dir_path, f\"crawled_jobs_{date.strftime('%Y-%m-%d')}.csv\")\n",
    "    print(csv_path)\n",
    "    \n",
    "    new_daily_data = pd.read_csv(csv_path)\n",
    "    new_daily_data = new_daily_data.pipe(standardize_job_urls)\n",
    "    # new_daily_data.loc[:,\"job_description\"] = new_daily_data.linkedin_url.map(\n",
    "    #     lambda job_link: crawl_jd(driver, job_link)\n",
    "    # )\n",
    "    if \"jd_recrawled\" not in new_daily_data.columns:\n",
    "        new_daily_data.loc[:,\"jd_recrawled\"] = False\n",
    "    for i, row in new_daily_data.iterrows():\n",
    "        if row.jd_recrawled:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            jd = crawl_jd(driver, row.linkedin_url)\n",
    "            new_daily_data.loc[i,\"job_description\"] = jd\n",
    "            new_daily_data.loc[i,\"jd_recrawled\"] = True\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        if (i+1)%10==0:\n",
    "            print(f\"...{i+1} of {len(new_daily_data)} jobs done.\")\n",
    "            new_daily_data.to_csv(csv_path, index=False)\n",
    "        \n",
    "    new_daily_data.to_csv(csv_path, index=False)\n",
    "    print(\"... CSV re-crawling done.\")\n",
    "    print()\n",
    "\n",
    "    date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c68d69-812d-4846-bcce-e61c4759d0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

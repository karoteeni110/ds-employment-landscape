{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a1edd-7adb-4bbf-a672-fbfb84d14e32",
   "metadata": {},
   "source": [
    "# Scrape Linkedin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d7fc9e-da88-4ef3-9af4-85ef9ecd6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper==2.11.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have installed the dependency\n",
    "! pip freeze | grep linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50695a9e-6b21-44db-825b-922a213bbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Chrome 114.0.5735.90 \n"
     ]
    }
   ],
   "source": [
    "! google-chrome-stable --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755f7ef-09a8-4caa-809f-551ba6949eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, Job, actions\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "\n",
    "class _JobSearch(JobSearch):\n",
    "    def __init__(self, final_url=None, **kwargs):\n",
    "        self.final_url = final_url\n",
    "        self.current_url = None\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def search(self, search_term: str, page_n) -> List[Job]:\n",
    "        if self.final_url is None:\n",
    "            self.current_url = os.path.join(self.base_url, \"search\") + f\"?keywords={urllib.parse.quote(search_term)}&refresh=true\"\n",
    "            self.driver.get(self.current_url)\n",
    "\n",
    "            # Get redirection URL\n",
    "            self.final_url = self.driver.current_url\n",
    "        else:\n",
    "            self.current_url = os.path.join(self.final_url, f\"&start={25*(page_n-1)}\")\n",
    "            self.driver.get(self.current_url)\n",
    "        \n",
    "        self.scroll_to_bottom()\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_listing_class_name = \"jobs-search-results-list\"\n",
    "        job_listing = self.wait_for_element_to_load(name=job_listing_class_name)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.3)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.6)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 1)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_results = []\n",
    "        for job_card in self.wait_for_all_elements_to_load(name=\"job-card-list\", base=job_listing):\n",
    "            job = self.scrape_job_card(job_card)\n",
    "            job_results.append(job)\n",
    "        return job_results\n",
    "\n",
    "def are_same(job1: Job, job2: Job):\n",
    "    if job1.job_title == job2.job_title and job1.company == job2.company:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612fd0-f5c4-4b3a-b2ee-578b132f471c",
   "metadata": {},
   "source": [
    "## 1. Scrape Job Search\n",
    "\n",
    "Scrape the first 20 pages of the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771e66e2-2364-4cdb-b4f1-f2abb4ec7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up the lower-level services for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\")\n",
    "job_search = _JobSearch(driver=driver, close_on_complete=False, scrape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d87d07e-659d-4abb-a058-e6d2c14e108e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Searching jobs... Keyword: data; Page 1/50'\n",
      "'FINISHED PAGE: 1'\n",
      "'Searching jobs... Keyword: data; Page 2/50'\n",
      "'FINISHED PAGE: 2'\n",
      "'Searching jobs... Keyword: data; Page 3/50'\n",
      "'FINISHED PAGE: 3'\n",
      "'Searching jobs... Keyword: data; Page 4/50'\n",
      "'FINISHED PAGE: 4'\n",
      "'Searching jobs... Keyword: data; Page 5/50'\n",
      "'FINISHED PAGE: 5'\n",
      "'Searching jobs... Keyword: data; Page 6/50'\n",
      "'FINISHED PAGE: 6'\n",
      "'Searching jobs... Keyword: data; Page 7/50'\n",
      "'FINISHED PAGE: 7'\n",
      "'Searching jobs... Keyword: data; Page 8/50'\n",
      "'FINISHED PAGE: 8'\n",
      "'Searching jobs... Keyword: data; Page 9/50'\n",
      "'FINISHED PAGE: 9'\n",
      "'Searching jobs... Keyword: data; Page 10/50'\n",
      "'FINISHED PAGE: 10'\n",
      "'Searching jobs... Keyword: data; Page 11/50'\n",
      "'FINISHED PAGE: 11'\n",
      "'Searching jobs... Keyword: data; Page 12/50'\n",
      "'FINISHED PAGE: 12'\n",
      "'Searching jobs... Keyword: data; Page 13/50'\n",
      "'FINISHED PAGE: 13'\n",
      "'Searching jobs... Keyword: data; Page 14/50'\n",
      "'FINISHED PAGE: 14'\n",
      "'Searching jobs... Keyword: data; Page 15/50'\n",
      "'FINISHED PAGE: 15'\n",
      "'Searching jobs... Keyword: data; Page 16/50'\n",
      "'FINISHED PAGE: 16'\n",
      "'Searching jobs... Keyword: data; Page 17/50'\n",
      "'FINISHED PAGE: 17'\n",
      "'Searching jobs... Keyword: data; Page 18/50'\n",
      "'FINISHED PAGE: 18'\n",
      "'Searching jobs... Keyword: data; Page 19/50'\n",
      "'FINISHED PAGE: 19'\n",
      "'Searching jobs... Keyword: data; Page 20/50'\n",
      "'FINISHED PAGE: 20'\n",
      "'Searching jobs... Keyword: data; Page 21/50'\n",
      "'FINISHED PAGE: 21'\n",
      "'Searching jobs... Keyword: data; Page 22/50'\n",
      "'SKIPPED PAGE: 22'\n",
      "'Searching jobs... Keyword: data; Page 23/50'\n",
      "'SKIPPED PAGE: 23'\n",
      "'Searching jobs... Keyword: data; Page 24/50'\n",
      "'SKIPPED PAGE: 24'\n",
      "'Searching jobs... Keyword: data; Page 25/50'\n",
      "'SKIPPED PAGE: 25'\n",
      "'Searching jobs... Keyword: data; Page 26/50'\n",
      "'SKIPPED PAGE: 26'\n",
      "'Searching jobs... Keyword: data; Page 27/50'\n",
      "'SKIPPED PAGE: 27'\n",
      "'Searching jobs... Keyword: data; Page 28/50'\n",
      "'SKIPPED PAGE: 28'\n",
      "'Searching jobs... Keyword: data; Page 29/50'\n",
      "'SKIPPED PAGE: 29'\n",
      "'Searching jobs... Keyword: data; Page 30/50'\n",
      "'SKIPPED PAGE: 30'\n",
      "'Searching jobs... Keyword: data; Page 31/50'\n",
      "'SKIPPED PAGE: 31'\n",
      "'Searching jobs... Keyword: data; Page 32/50'\n",
      "'SKIPPED PAGE: 32'\n",
      "'Searching jobs... Keyword: data; Page 33/50'\n",
      "'SKIPPED PAGE: 33'\n",
      "'Searching jobs... Keyword: data; Page 34/50'\n",
      "'SKIPPED PAGE: 34'\n",
      "'Searching jobs... Keyword: data; Page 35/50'\n",
      "'SKIPPED PAGE: 35'\n",
      "'Searching jobs... Keyword: data; Page 36/50'\n",
      "'SKIPPED PAGE: 36'\n",
      "'Searching jobs... Keyword: data; Page 37/50'\n",
      "'SKIPPED PAGE: 37'\n",
      "'Searching jobs... Keyword: data; Page 38/50'\n",
      "'SKIPPED PAGE: 38'\n",
      "'Searching jobs... Keyword: data; Page 39/50'\n",
      "'SKIPPED PAGE: 39'\n",
      "'Searching jobs... Keyword: data; Page 40/50'\n",
      "'SKIPPED PAGE: 40'\n",
      "'Searching jobs... Keyword: data; Page 41/50'\n",
      "'SKIPPED PAGE: 41'\n",
      "'Searching jobs... Keyword: data; Page 42/50'\n",
      "'FINISHED PAGE: 42'\n",
      "'Searching jobs... Keyword: data; Page 43/50'\n",
      "'FINISHED PAGE: 43'\n",
      "'Searching jobs... Keyword: data; Page 44/50'\n",
      "'FINISHED PAGE: 44'\n",
      "'Searching jobs... Keyword: data; Page 45/50'\n",
      "'FINISHED PAGE: 45'\n",
      "'Searching jobs... Keyword: data; Page 46/50'\n",
      "'FINISHED PAGE: 46'\n",
      "'Searching jobs... Keyword: data; Page 47/50'\n",
      "'FINISHED PAGE: 47'\n",
      "'Searching jobs... Keyword: data; Page 48/50'\n",
      "'FINISHED PAGE: 48'\n",
      "'Searching jobs... Keyword: data; Page 49/50'\n",
      "'FINISHED PAGE: 49'\n",
      "'Searching jobs... Keyword: data; Page 50/50'\n",
      "'FINISHED PAGE: 50'\n",
      "CPU times: user 3.94 s, sys: 655 ms, total: 4.6 s\n",
      "Wall time: 24min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "N_PAGES = 50\n",
    "SEARCH_KEYWORD = \"data\"\n",
    "\n",
    "jobs = []\n",
    "for page_n in range(1, N_PAGES+1):\n",
    "    pprint(f\"Searching jobs... Keyword: {SEARCH_KEYWORD}; Page {page_n}/{N_PAGES}\")\n",
    "    try:\n",
    "        new_batch = job_search.search(SEARCH_KEYWORD, page_n)\n",
    "    except TimeoutException:\n",
    "        pprint(f\"SKIPPED PAGE: {page_n}\")\n",
    "        continue\n",
    "\n",
    "    # Check if the new batch of jobs are duplicates, \n",
    "    # which means we have gone through all the pages and should quit scraping.\n",
    "    if jobs and are_same(new_batch[0], jobs[0]):\n",
    "        pprint(\"Found duplicate results! All the pages have been scraped. Quiting...\")\n",
    "        break\n",
    "        \n",
    "    jobs.extend(new_batch)\n",
    "    pprint(f\"FINISHED PAGE: {page_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ca956d-a4cb-443d-8d9d-7bbc96e6e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500ab9ca-d346-4b90-abb0-fafdffefca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl temporarily\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "with open(f\"../data/tmp/{fname}\", \"wb\") as f:\n",
    "    dicted_jobs = [job.to_dict() for job in jobs]\n",
    "    pickle.dump(dicted_jobs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f02c2-cd94-4e16-bb3e-3c3c4b761913",
   "metadata": {},
   "source": [
    "## 2. Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17c1166-5c4d-4ee4-a684-10c0535f32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job, actions\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "        self.job_title = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-unified-top-card__job-title')]\").text.strip()\n",
    "        self.company = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a[1]\").text.strip()\n",
    "        self.company_linkedin_url = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a\").get_attribute(\"href\")\n",
    "        self.location = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//*\").text.strip()\n",
    "        self.posted_date = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//span[3]\").text.strip()\n",
    "        self.job_type_1 = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')]/span\").text.strip()\n",
    "        self.job_description = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\").text.strip()\n",
    "        \n",
    "        try:\n",
    "            self.required_skills = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][1]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.required_skills += self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][2]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.job_type_2 = self.wait_for_element_to_load(by=By.XPATH, name=\"(//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')])[2]/span\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.job_type_2 = \"\"\n",
    "            \n",
    "        try:\n",
    "            self.applicant_count = self.wait_for_element_to_load(by=By.XPATH, name=\"jobs-unified-top-card__applicant-count\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.applicant_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.benefits = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'salary-main-rail-card')]\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.benefits = \"\"\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0690bf7-4c55-434f-a72b-fae89fd21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010a69b-bf8d-434e-ba07-641b09ff211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up low-level servies for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5236-f815-4a94-b6b4-f0b368132f1e",
   "metadata": {},
   "source": [
    "Ignore the error logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b8e6e6-ed7c-4f70-8f48-8e5b463c681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "\n",
    "with open(f\"../data/tmp/{fname}\", \"rb\") as f:\n",
    "    jobs = pickle.load(f)\n",
    "\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10b00c9-d1d6-4723-89a1-7823fd0f4c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 188/373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55ee883fe4e3 <unknown>\n",
      "#1 0x55ee8812dc76 <unknown>\n",
      "#2 0x55ee88169c96 <unknown>\n",
      "#3 0x55ee88169dc1 <unknown>\n",
      "#4 0x55ee881a37f4 <unknown>\n",
      "#5 0x55ee8818903d <unknown>\n",
      "#6 0x55ee881a130e <unknown>\n",
      "#7 0x55ee88188de3 <unknown>\n",
      "#8 0x55ee8815e2dd <unknown>\n",
      "#9 0x55ee8815f34e <unknown>\n",
      "#10 0x55ee883be3e4 <unknown>\n",
      "#11 0x55ee883c23d7 <unknown>\n",
      "#12 0x55ee883ccb20 <unknown>\n",
      "#13 0x55ee883c3023 <unknown>\n",
      "#14 0x55ee883911aa <unknown>\n",
      "#15 0x55ee883e76b8 <unknown>\n",
      "#16 0x55ee883e7847 <unknown>\n",
      "#17 0x55ee883f7243 <unknown>\n",
      "#18 0x7f55f0094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 189/373\n",
      "Crawling... Jobs 190/373\n",
      "Crawling... Jobs 191/373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55ee883fe4e3 <unknown>\n",
      "#1 0x55ee8812dc76 <unknown>\n",
      "#2 0x55ee88169c96 <unknown>\n",
      "#3 0x55ee88169dc1 <unknown>\n",
      "#4 0x55ee881a37f4 <unknown>\n",
      "#5 0x55ee8818903d <unknown>\n",
      "#6 0x55ee881a130e <unknown>\n",
      "#7 0x55ee88188de3 <unknown>\n",
      "#8 0x55ee8815e2dd <unknown>\n",
      "#9 0x55ee8815f34e <unknown>\n",
      "#10 0x55ee883be3e4 <unknown>\n",
      "#11 0x55ee883c23d7 <unknown>\n",
      "#12 0x55ee883ccb20 <unknown>\n",
      "#13 0x55ee883c3023 <unknown>\n",
      "#14 0x55ee883911aa <unknown>\n",
      "#15 0x55ee883e76b8 <unknown>\n",
      "#16 0x55ee883e7847 <unknown>\n",
      "#17 0x55ee883f7243 <unknown>\n",
      "#18 0x7f55f0094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 192/373\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_CONNECTION_REFUSED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x55ee883fe4e3 <unknown>\n#1 0x55ee8812dc76 <unknown>\n#2 0x55ee88125c7f <unknown>\n#3 0x55ee88117ca2 <unknown>\n#4 0x55ee88119412 <unknown>\n#5 0x55ee881180ca <unknown>\n#6 0x55ee88117168 <unknown>\n#7 0x55ee88116fa0 <unknown>\n#8 0x55ee881159bf <unknown>\n#9 0x55ee88115fed <unknown>\n#10 0x55ee8812fb06 <unknown>\n#11 0x55ee881a19e5 <unknown>\n#12 0x55ee88189012 <unknown>\n#13 0x55ee881a130e <unknown>\n#14 0x55ee88188de3 <unknown>\n#15 0x55ee8815e2dd <unknown>\n#16 0x55ee8815f34e <unknown>\n#17 0x55ee883be3e4 <unknown>\n#18 0x55ee883c23d7 <unknown>\n#19 0x55ee883ccb20 <unknown>\n#20 0x55ee883c3023 <unknown>\n#21 0x55ee883911aa <unknown>\n#22 0x55ee883e76b8 <unknown>\n#23 0x55ee883e7847 <unknown>\n#24 0x55ee883f7243 <unknown>\n#25 0x7f55f0094ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36m_Job.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:40\u001b[0m, in \u001b[0;36mJob.__init__\u001b[0;34m(self, linkedin_url, job_title, company, company_linkedin_url, location, posted_date, applicant_count, job_description, benefits, driver, close_on_complete, scrape)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m benefits\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scrape:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:47\u001b[0m, in \u001b[0;36mJob.scrape\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed_in():\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_logged_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis part is not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36m_Job.scrape_logged_in\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_logged_in\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinkedin_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfocus()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_element_to_load(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mXPATH, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//*[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobs-unified-top-card__job-title\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_CONNECTION_REFUSED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x55ee883fe4e3 <unknown>\n#1 0x55ee8812dc76 <unknown>\n#2 0x55ee88125c7f <unknown>\n#3 0x55ee88117ca2 <unknown>\n#4 0x55ee88119412 <unknown>\n#5 0x55ee881180ca <unknown>\n#6 0x55ee88117168 <unknown>\n#7 0x55ee88116fa0 <unknown>\n#8 0x55ee881159bf <unknown>\n#9 0x55ee88115fed <unknown>\n#10 0x55ee8812fb06 <unknown>\n#11 0x55ee881a19e5 <unknown>\n#12 0x55ee88189012 <unknown>\n#13 0x55ee881a130e <unknown>\n#14 0x55ee88188de3 <unknown>\n#15 0x55ee8815e2dd <unknown>\n#16 0x55ee8815f34e <unknown>\n#17 0x55ee883be3e4 <unknown>\n#18 0x55ee883c23d7 <unknown>\n#19 0x55ee883ccb20 <unknown>\n#20 0x55ee883c3023 <unknown>\n#21 0x55ee883911aa <unknown>\n#22 0x55ee883e76b8 <unknown>\n#23 0x55ee883e7847 <unknown>\n#24 0x55ee883f7243 <unknown>\n#25 0x7f55f0094ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from time import sleep\n",
    "\n",
    "N_JOBS = len(jobs)\n",
    "\n",
    "crawled_jobs = []\n",
    "for i, job in enumerate(jobs):\n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except StaleElementReferenceException:\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad0dbf8-cba2-42bc-a496-57345dde2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260db594-e6f7-4ebc-9294-e132e9ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]\n",
    "                              ).drop(columns=[\"driver\"]\n",
    "                              ).drop_duplicates(\"linkedin_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2d9665-a746-4a98-a1f2-7446de35ac9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Analysis, Data Science, Machine Learning,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3735986015/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  2 week...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst, Ads</td>\n",
       "      <td>Data Analysis, Python (Programming Language), ...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3702242885/...</td>\n",
       "      <td>Rovio Entertainment Corporation</td>\n",
       "      <td>https://www.linkedin.com/company/rovio/life</td>\n",
       "      <td>Rovio Entertainment Corporation · Helsinki, Uu...</td>\n",
       "      <td>Reposted  1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAt Rovio you will get to work w...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JVM Performance and Tuning Engineer</td>\n",
       "      <td>Business Logic, Garbage Collection, Honeycomb,...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3734708994/...</td>\n",
       "      <td>RELEX Solutions</td>\n",
       "      <td>https://www.linkedin.com/company/relexsolution...</td>\n",
       "      <td>RELEX Solutions · Finland  2 weeks ago  · 10 a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nRELEX Solutions create cutting-...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer (Level Up)</td>\n",
       "      <td>Data Warehousing, Finnish, and SQLData Visuali...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3744740320/...</td>\n",
       "      <td>Loihde Advance</td>\n",
       "      <td>https://www.linkedin.com/company/loihdeadvance...</td>\n",
       "      <td>Loihde Advance · Uusimaa, Finland  1 week ago ...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOnko sinulle jo kertynyt jo väh...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science - Machine Learning Engineer</td>\n",
       "      <td>Artificial Intelligence (AI), Computer Science...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3629670334/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland Reposted  6 ...</td>\n",
       "      <td>Reposted  6 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nTeam purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Financial Crime Prevention Senior/Master Exper...</td>\n",
       "      <td>Analytical SkillsBudgeting, Communication, Cor...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3720314180/...</td>\n",
       "      <td>Nordea</td>\n",
       "      <td>https://www.linkedin.com/company/nordea/life</td>\n",
       "      <td>Nordea · Helsinki, Uusimaa, Finland Reposted  ...</td>\n",
       "      <td>Reposted  1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob ID: 19410 \\n As a Senior Ex...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Flutter Engineer, Merchant Group</td>\n",
       "      <td>Mobile Telephony</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737949780/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland  2 weeks ago...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nWolt is look...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Senior Financial Analyst, New Build</td>\n",
       "      <td>Analytical SkillsCapital Allocation, Finance, ...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3726702476/...</td>\n",
       "      <td>Royal Caribbean Group</td>\n",
       "      <td>https://www.linkedin.com/company/royal-caribbe...</td>\n",
       "      <td>Royal Caribbean Group · Turku, Southwest Finla...</td>\n",
       "      <td>Reposted  3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nPosition Summary\\n\\nThe purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Lead Software Engineer (AI Product Lab)</td>\n",
       "      <td>Back-End Web Development, Data Science, Python...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3628841523/...</td>\n",
       "      <td>Smartly.io</td>\n",
       "      <td>https://www.linkedin.com/company/smartly-io/life</td>\n",
       "      <td>Smartly.io · Helsinki, Uusimaa, Finland Repost...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJoin our dynamic Media Intellig...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Project Finance Controller</td>\n",
       "      <td>Cash Flow, Finance, Financial Performance, For...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3703119167/...</td>\n",
       "      <td>GE Renewable Energy</td>\n",
       "      <td>https://www.linkedin.com/company/gerenewableen...</td>\n",
       "      <td>GE Renewable Energy · Tampere, Pirkanmaa, Finl...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description Summary We are ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  \\\n",
       "0                                       Data Scientist   \n",
       "1                             Senior Data Analyst, Ads   \n",
       "2                  JVM Performance and Tuning Engineer   \n",
       "3                             Data Engineer (Level Up)   \n",
       "4             Data Science - Machine Learning Engineer   \n",
       "..                                                 ...   \n",
       "186  Financial Crime Prevention Senior/Master Exper...   \n",
       "187                   Flutter Engineer, Merchant Group   \n",
       "188                Senior Financial Analyst, New Build   \n",
       "189            Lead Software Engineer (AI Product Lab)   \n",
       "190                         Project Finance Controller   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    Data Analysis, Data Science, Machine Learning,...     Hybrid  Full-time   \n",
       "1    Data Analysis, Python (Programming Language), ...     Hybrid  Full-time   \n",
       "2    Business Logic, Garbage Collection, Honeycomb,...     Remote  Full-time   \n",
       "3    Data Warehousing, Finnish, and SQLData Visuali...     Hybrid  Full-time   \n",
       "4    Artificial Intelligence (AI), Computer Science...     Remote  Full-time   \n",
       "..                                                 ...        ...        ...   \n",
       "186  Analytical SkillsBudgeting, Communication, Cor...    On-site  Full-time   \n",
       "187                                   Mobile Telephony     Remote  Full-time   \n",
       "188  Analytical SkillsCapital Allocation, Finance, ...    On-site  Full-time   \n",
       "189  Back-End Web Development, Data Science, Python...     Remote  Full-time   \n",
       "190  Cash Flow, Finance, Financial Performance, For...  Full-time              \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3735986015/...   \n",
       "1    https://www.linkedin.com/jobs/view/3702242885/...   \n",
       "2    https://www.linkedin.com/jobs/view/3734708994/...   \n",
       "3    https://www.linkedin.com/jobs/view/3744740320/...   \n",
       "4    https://www.linkedin.com/jobs/view/3629670334/...   \n",
       "..                                                 ...   \n",
       "186  https://www.linkedin.com/jobs/view/3720314180/...   \n",
       "187  https://www.linkedin.com/jobs/view/3737949780/...   \n",
       "188  https://www.linkedin.com/jobs/view/3726702476/...   \n",
       "189  https://www.linkedin.com/jobs/view/3628841523/...   \n",
       "190  https://www.linkedin.com/jobs/view/3703119167/...   \n",
       "\n",
       "                             company  \\\n",
       "0                          MedEngine   \n",
       "1    Rovio Entertainment Corporation   \n",
       "2                    RELEX Solutions   \n",
       "3                     Loihde Advance   \n",
       "4                               Wolt   \n",
       "..                               ...   \n",
       "186                           Nordea   \n",
       "187                             Wolt   \n",
       "188            Royal Caribbean Group   \n",
       "189                       Smartly.io   \n",
       "190              GE Renewable Energy   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0      https://www.linkedin.com/company/medengine/life   \n",
       "1          https://www.linkedin.com/company/rovio/life   \n",
       "2    https://www.linkedin.com/company/relexsolution...   \n",
       "3    https://www.linkedin.com/company/loihdeadvance...   \n",
       "4        https://www.linkedin.com/company/wolt-oy/life   \n",
       "..                                                 ...   \n",
       "186       https://www.linkedin.com/company/nordea/life   \n",
       "187      https://www.linkedin.com/company/wolt-oy/life   \n",
       "188  https://www.linkedin.com/company/royal-caribbe...   \n",
       "189   https://www.linkedin.com/company/smartly-io/life   \n",
       "190  https://www.linkedin.com/company/gerenewableen...   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    MedEngine · Helsinki, Uusimaa, Finland  2 week...            2 weeks ago   \n",
       "1    Rovio Entertainment Corporation · Helsinki, Uu...   Reposted  1 week ago   \n",
       "2    RELEX Solutions · Finland  2 weeks ago  · 10 a...            2 weeks ago   \n",
       "3    Loihde Advance · Uusimaa, Finland  1 week ago ...             1 week ago   \n",
       "4    Wolt · Helsinki, Uusimaa, Finland Reposted  6 ...  Reposted  6 hours ago   \n",
       "..                                                 ...                    ...   \n",
       "186  Nordea · Helsinki, Uusimaa, Finland Reposted  ...   Reposted  1 week ago   \n",
       "187  Wolt · Helsinki, Uusimaa, Finland  2 weeks ago...            2 weeks ago   \n",
       "188  Royal Caribbean Group · Turku, Southwest Finla...   Reposted  3 days ago   \n",
       "189  Smartly.io · Helsinki, Uusimaa, Finland Repost...  Reposted  2 weeks ago   \n",
       "190  GE Renewable Energy · Tampere, Pirkanmaa, Finl...  Reposted  2 weeks ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "1                  0  About the job\\nAt Rovio you will get to work w...   \n",
       "2                  0  About the job\\nRELEX Solutions create cutting-...   \n",
       "3                  0  About the job\\nOnko sinulle jo kertynyt jo väh...   \n",
       "4                  0  About the job\\nJob Description\\n\\nTeam purpose...   \n",
       "..               ...                                                ...   \n",
       "186                0  About the job\\nJob ID: 19410 \\n As a Senior Ex...   \n",
       "187                0  About the job\\nJob Description\\n\\nWolt is look...   \n",
       "188                0  About the job\\nPosition Summary\\n\\nThe purpose...   \n",
       "189                0  About the job\\nJoin our dynamic Media Intellig...   \n",
       "190                0  About the job\\nJob Description Summary We are ...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "186           \n",
       "187           \n",
       "188           \n",
       "189           \n",
       "190           \n",
       "\n",
       "[191 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b58dfc6-3ab7-416e-834f-525cb464aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crawled_jobs.to_csv(f\"../data/crawled_jobs_1-{len(crawled_jobs}_checkpoint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568898a5-df54-4afa-85ff-3b858f901e30",
   "metadata": {},
   "source": [
    "### 2.1 Continue from the failed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e352c75e-aaa9-4bd8-ac86-add24d96cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# In case session expiration\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50eff973-e5db-40c0-9a73-24e02e836e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 358/373\n",
      "Crawling... Jobs 359/373\n",
      "Crawling... Jobs 360/373\n",
      "Crawling... Jobs 361/373\n",
      "Crawling... Jobs 362/373\n",
      "Crawling... Jobs 363/373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x556d8beff4e3 <unknown>\n",
      "#1 0x556d8bc2ec76 <unknown>\n",
      "#2 0x556d8bc6ac96 <unknown>\n",
      "#3 0x556d8bc6adc1 <unknown>\n",
      "#4 0x556d8bca47f4 <unknown>\n",
      "#5 0x556d8bc8a03d <unknown>\n",
      "#6 0x556d8bca230e <unknown>\n",
      "#7 0x556d8bc89de3 <unknown>\n",
      "#8 0x556d8bc5f2dd <unknown>\n",
      "#9 0x556d8bc6034e <unknown>\n",
      "#10 0x556d8bebf3e4 <unknown>\n",
      "#11 0x556d8bec33d7 <unknown>\n",
      "#12 0x556d8becdb20 <unknown>\n",
      "#13 0x556d8bec4023 <unknown>\n",
      "#14 0x556d8be921aa <unknown>\n",
      "#15 0x556d8bee86b8 <unknown>\n",
      "#16 0x556d8bee8847 <unknown>\n",
      "#17 0x556d8bef8243 <unknown>\n",
      "#18 0x7fe1c6494ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 364/373\n",
      "Crawling... Jobs 365/373\n",
      "Crawling... Jobs 366/373\n",
      "Crawling... Jobs 367/373\n",
      "Crawling... Jobs 368/373\n",
      "Crawling... Jobs 369/373\n",
      "Crawling... Jobs 370/373\n",
      "Crawling... Jobs 371/373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x556d8beff4e3 <unknown>\n",
      "#1 0x556d8bc2ec76 <unknown>\n",
      "#2 0x556d8bc6ac96 <unknown>\n",
      "#3 0x556d8bc6adc1 <unknown>\n",
      "#4 0x556d8bca47f4 <unknown>\n",
      "#5 0x556d8bc8a03d <unknown>\n",
      "#6 0x556d8bca230e <unknown>\n",
      "#7 0x556d8bc89de3 <unknown>\n",
      "#8 0x556d8bc5f2dd <unknown>\n",
      "#9 0x556d8bc6034e <unknown>\n",
      "#10 0x556d8bebf3e4 <unknown>\n",
      "#11 0x556d8bec33d7 <unknown>\n",
      "#12 0x556d8becdb20 <unknown>\n",
      "#13 0x556d8bec4023 <unknown>\n",
      "#14 0x556d8be921aa <unknown>\n",
      "#15 0x556d8bee86b8 <unknown>\n",
      "#16 0x556d8bee8847 <unknown>\n",
      "#17 0x556d8bef8243 <unknown>\n",
      "#18 0x7fe1c6494ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 372/373\n",
      "Crawling... Jobs 373/373\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "\n",
    "CONTINUE_FROM = 226\n",
    "\n",
    "for i, job in enumerate(jobs):\n",
    "    if i+1<CONTINUE_FROM:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "    crawled_jobs.append(_crawled_job)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f3595eb-d170-4e5a-b7d3-d86b8480a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Analysis, Data Science, Machine Learning,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3735986015/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  2 week...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst, Ads</td>\n",
       "      <td>Data Analysis, Python (Programming Language), ...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3702242885/...</td>\n",
       "      <td>Rovio Entertainment Corporation</td>\n",
       "      <td>https://www.linkedin.com/company/rovio/life</td>\n",
       "      <td>Rovio Entertainment Corporation · Helsinki, Uu...</td>\n",
       "      <td>Reposted  1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAt Rovio you will get to work w...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JVM Performance and Tuning Engineer</td>\n",
       "      <td>Business Logic, Garbage Collection, Honeycomb,...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3734708994/...</td>\n",
       "      <td>RELEX Solutions</td>\n",
       "      <td>https://www.linkedin.com/company/relexsolution...</td>\n",
       "      <td>RELEX Solutions · Finland  2 weeks ago  · 10 a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nRELEX Solutions create cutting-...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer (Level Up)</td>\n",
       "      <td>Data Warehousing, Finnish, and SQLData Visuali...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3744740320/...</td>\n",
       "      <td>Loihde Advance</td>\n",
       "      <td>https://www.linkedin.com/company/loihdeadvance...</td>\n",
       "      <td>Loihde Advance · Uusimaa, Finland  1 week ago ...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOnko sinulle jo kertynyt jo väh...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science - Machine Learning Engineer</td>\n",
       "      <td>Artificial Intelligence (AI), Computer Science...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3629670334/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland Reposted  6 ...</td>\n",
       "      <td>Reposted  6 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nTeam purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Machine Learning Engineer - MLOps</td>\n",
       "      <td>Artificial Intelligence (AI), Data Mining, Dat...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3750358338/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland  2 days ago ...</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nTeam purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>Databricks data engineer</td>\n",
       "      <td>Data Analytics, Data Engineering, Data Science...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3729429877/...</td>\n",
       "      <td>Accenture Nordics</td>\n",
       "      <td>https://www.linkedin.com/company/accenture-nor...</td>\n",
       "      <td>Accenture Nordics · Helsinki, Uusimaa, Finland...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nHaemme Databricks data engineer...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>Lead Quantitative Risk Analyst (Data Analytics...</td>\n",
       "      <td>Business Requirements, Data Reconciliation, Fu...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3733079663/...</td>\n",
       "      <td>Nordea</td>\n",
       "      <td>https://www.linkedin.com/company/nordea/life</td>\n",
       "      <td>Nordea · Helsinki, Uusimaa, Finland  2 weeks a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob ID: 18802 \\nWe are looking ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Cloud Ops Engineer</td>\n",
       "      <td>Cloud Computing and Microsoft AzureAzure Cosmo...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3694191393/...</td>\n",
       "      <td>Cloud1 Oy</td>\n",
       "      <td>https://www.linkedin.com/company/cloud1-oy/life</td>\n",
       "      <td>Cloud1 Oy · Helsinki, Uusimaa, Finland Reposte...</td>\n",
       "      <td>Reposted  3 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nCloud1 hakee kokenutta teknistä...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Data Engineer, OP Life Insurance</td>\n",
       "      <td>Data Analytics, Data Engineering, Data Warehou...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3749553229/...</td>\n",
       "      <td>OP Financial Group</td>\n",
       "      <td>https://www.linkedin.com/company/op-financial-...</td>\n",
       "      <td>OP Financial Group · Helsinki, Uusimaa, Finlan...</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAre you ready to embark on an e...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  \\\n",
       "0                                       Data Scientist   \n",
       "1                             Senior Data Analyst, Ads   \n",
       "2                  JVM Performance and Tuning Engineer   \n",
       "3                             Data Engineer (Level Up)   \n",
       "4             Data Science - Machine Learning Engineer   \n",
       "..                                                 ...   \n",
       "368                  Machine Learning Engineer - MLOps   \n",
       "369                           Databricks data engineer   \n",
       "370  Lead Quantitative Risk Analyst (Data Analytics...   \n",
       "371                                 Cloud Ops Engineer   \n",
       "372                   Data Engineer, OP Life Insurance   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    Data Analysis, Data Science, Machine Learning,...     Hybrid  Full-time   \n",
       "1    Data Analysis, Python (Programming Language), ...     Hybrid  Full-time   \n",
       "2    Business Logic, Garbage Collection, Honeycomb,...     Remote  Full-time   \n",
       "3    Data Warehousing, Finnish, and SQLData Visuali...     Hybrid  Full-time   \n",
       "4    Artificial Intelligence (AI), Computer Science...     Remote  Full-time   \n",
       "..                                                 ...        ...        ...   \n",
       "368  Artificial Intelligence (AI), Data Mining, Dat...     Remote  Full-time   \n",
       "369  Data Analytics, Data Engineering, Data Science...     Hybrid  Full-time   \n",
       "370  Business Requirements, Data Reconciliation, Fu...    On-site  Full-time   \n",
       "371  Cloud Computing and Microsoft AzureAzure Cosmo...    On-site  Full-time   \n",
       "372  Data Analytics, Data Engineering, Data Warehou...     Hybrid  Full-time   \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3735986015/...   \n",
       "1    https://www.linkedin.com/jobs/view/3702242885/...   \n",
       "2    https://www.linkedin.com/jobs/view/3734708994/...   \n",
       "3    https://www.linkedin.com/jobs/view/3744740320/...   \n",
       "4    https://www.linkedin.com/jobs/view/3629670334/...   \n",
       "..                                                 ...   \n",
       "368  https://www.linkedin.com/jobs/view/3750358338/...   \n",
       "369  https://www.linkedin.com/jobs/view/3729429877/...   \n",
       "370  https://www.linkedin.com/jobs/view/3733079663/...   \n",
       "371  https://www.linkedin.com/jobs/view/3694191393/...   \n",
       "372  https://www.linkedin.com/jobs/view/3749553229/...   \n",
       "\n",
       "                             company  \\\n",
       "0                          MedEngine   \n",
       "1    Rovio Entertainment Corporation   \n",
       "2                    RELEX Solutions   \n",
       "3                     Loihde Advance   \n",
       "4                               Wolt   \n",
       "..                               ...   \n",
       "368                             Wolt   \n",
       "369                Accenture Nordics   \n",
       "370                           Nordea   \n",
       "371                        Cloud1 Oy   \n",
       "372               OP Financial Group   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0      https://www.linkedin.com/company/medengine/life   \n",
       "1          https://www.linkedin.com/company/rovio/life   \n",
       "2    https://www.linkedin.com/company/relexsolution...   \n",
       "3    https://www.linkedin.com/company/loihdeadvance...   \n",
       "4        https://www.linkedin.com/company/wolt-oy/life   \n",
       "..                                                 ...   \n",
       "368      https://www.linkedin.com/company/wolt-oy/life   \n",
       "369  https://www.linkedin.com/company/accenture-nor...   \n",
       "370       https://www.linkedin.com/company/nordea/life   \n",
       "371    https://www.linkedin.com/company/cloud1-oy/life   \n",
       "372  https://www.linkedin.com/company/op-financial-...   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    MedEngine · Helsinki, Uusimaa, Finland  2 week...            2 weeks ago   \n",
       "1    Rovio Entertainment Corporation · Helsinki, Uu...   Reposted  1 week ago   \n",
       "2    RELEX Solutions · Finland  2 weeks ago  · 10 a...            2 weeks ago   \n",
       "3    Loihde Advance · Uusimaa, Finland  1 week ago ...             1 week ago   \n",
       "4    Wolt · Helsinki, Uusimaa, Finland Reposted  6 ...  Reposted  6 hours ago   \n",
       "..                                                 ...                    ...   \n",
       "368  Wolt · Helsinki, Uusimaa, Finland  2 days ago ...             2 days ago   \n",
       "369  Accenture Nordics · Helsinki, Uusimaa, Finland...  Reposted  2 weeks ago   \n",
       "370  Nordea · Helsinki, Uusimaa, Finland  2 weeks a...            2 weeks ago   \n",
       "371  Cloud1 Oy · Helsinki, Uusimaa, Finland Reposte...  Reposted  3 weeks ago   \n",
       "372  OP Financial Group · Helsinki, Uusimaa, Finlan...             3 days ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "1                  0  About the job\\nAt Rovio you will get to work w...   \n",
       "2                  0  About the job\\nRELEX Solutions create cutting-...   \n",
       "3                  0  About the job\\nOnko sinulle jo kertynyt jo väh...   \n",
       "4                  0  About the job\\nJob Description\\n\\nTeam purpose...   \n",
       "..               ...                                                ...   \n",
       "368                0  About the job\\nJob Description\\n\\nTeam purpose...   \n",
       "369                0  About the job\\nHaemme Databricks data engineer...   \n",
       "370                0  About the job\\nJob ID: 18802 \\nWe are looking ...   \n",
       "371                0  About the job\\nCloud1 hakee kokenutta teknistä...   \n",
       "372                0  About the job\\nAre you ready to embark on an e...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "368           \n",
       "369           \n",
       "370           \n",
       "371           \n",
       "372           \n",
       "\n",
       "[373 rows x 12 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]).drop(columns=[\"driver\"]).drop_duplicates(\"linkedin_url\")\n",
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bf2bda0-7b3c-425d-87a4-0420c1e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"../data/crawled_jobs_{current_date}.csv\"\n",
    "\n",
    "df_crawled_jobs.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0d8b-2cb2-4217-ab99-6ece5a995d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94094-a7ba-4dd8-96d6-17b734384714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

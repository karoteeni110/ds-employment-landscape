{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a1edd-7adb-4bbf-a672-fbfb84d14e32",
   "metadata": {},
   "source": [
    "# Scrape Linkedin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d7fc9e-da88-4ef3-9af4-85ef9ecd6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper==2.11.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have installed the dependency\n",
    "! pip freeze | grep linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755f7ef-09a8-4caa-809f-551ba6949eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, Job, actions\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "\n",
    "class _JobSearch(JobSearch):\n",
    "    def __init__(self, final_url=None, **kwargs):\n",
    "        self.final_url = final_url\n",
    "        self.current_url = None\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def search(self, search_term: str, page_n) -> List[Job]:\n",
    "        if self.final_url is None:\n",
    "            self.current_url = os.path.join(self.base_url, \"search\") + f\"?keywords={urllib.parse.quote(search_term)}&refresh=true\"\n",
    "            self.driver.get(self.current_url)\n",
    "\n",
    "            # Get redirection URL\n",
    "            self.final_url = self.driver.current_url\n",
    "        else:\n",
    "            self.current_url = os.path.join(self.final_url, f\"&start={25*(page_n-1)}\")\n",
    "            self.driver.get(self.current_url)\n",
    "        \n",
    "        self.scroll_to_bottom()\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_listing_class_name = \"jobs-search-results-list\"\n",
    "        job_listing = self.wait_for_element_to_load(name=job_listing_class_name)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.3)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.6)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 1)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_results = []\n",
    "        for job_card in self.wait_for_all_elements_to_load(name=\"job-card-list\", base=job_listing):\n",
    "            job = self.scrape_job_card(job_card)\n",
    "            job_results.append(job)\n",
    "        return job_results\n",
    "\n",
    "def are_same(job1: Job, job2: Job):\n",
    "    if job1.job_title == job2.job_title and job1.company == job2.company:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612fd0-f5c4-4b3a-b2ee-578b132f471c",
   "metadata": {},
   "source": [
    "## 1. Scrape Job Search\n",
    "\n",
    "Scrape the first 20 pages of the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771e66e2-2364-4cdb-b4f1-f2abb4ec7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up the lower-level services for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\")\n",
    "job_search = _JobSearch(driver=driver, close_on_complete=False, scrape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d87d07e-659d-4abb-a058-e6d2c14e108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Searching jobs... Keyword: data; Page 1/22'\n",
      "'FINISHED PAGE: 1'\n",
      "'Searching jobs... Keyword: data; Page 2/22'\n",
      "'FINISHED PAGE: 2'\n",
      "'Searching jobs... Keyword: data; Page 3/22'\n",
      "'FINISHED PAGE: 3'\n",
      "'Searching jobs... Keyword: data; Page 4/22'\n",
      "'FINISHED PAGE: 4'\n",
      "'Searching jobs... Keyword: data; Page 5/22'\n",
      "'FINISHED PAGE: 5'\n",
      "'Searching jobs... Keyword: data; Page 6/22'\n",
      "'FINISHED PAGE: 6'\n",
      "'Searching jobs... Keyword: data; Page 7/22'\n",
      "'FINISHED PAGE: 7'\n",
      "'Searching jobs... Keyword: data; Page 8/22'\n",
      "'FINISHED PAGE: 8'\n",
      "'Searching jobs... Keyword: data; Page 9/22'\n",
      "'FINISHED PAGE: 9'\n",
      "'Searching jobs... Keyword: data; Page 10/22'\n",
      "'FINISHED PAGE: 10'\n",
      "'Searching jobs... Keyword: data; Page 11/22'\n",
      "'FINISHED PAGE: 11'\n",
      "'Searching jobs... Keyword: data; Page 12/22'\n",
      "'FINISHED PAGE: 12'\n",
      "'Searching jobs... Keyword: data; Page 13/22'\n",
      "'FINISHED PAGE: 13'\n",
      "'Searching jobs... Keyword: data; Page 14/22'\n",
      "'FINISHED PAGE: 14'\n",
      "'Searching jobs... Keyword: data; Page 15/22'\n",
      "'FINISHED PAGE: 15'\n",
      "'Searching jobs... Keyword: data; Page 16/22'\n",
      "'FINISHED PAGE: 16'\n",
      "'Searching jobs... Keyword: data; Page 17/22'\n",
      "'FINISHED PAGE: 17'\n",
      "'Searching jobs... Keyword: data; Page 18/22'\n",
      "'FINISHED PAGE: 18'\n",
      "'Searching jobs... Keyword: data; Page 19/22'\n",
      "'FINISHED PAGE: 19'\n",
      "'Searching jobs... Keyword: data; Page 20/22'\n",
      "'FINISHED PAGE: 20'\n",
      "'Searching jobs... Keyword: data; Page 21/22'\n",
      "'FINISHED PAGE: 21'\n",
      "'Searching jobs... Keyword: data; Page 22/22'\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m, in \u001b[0;36m_JobSearch.search\u001b[0;34m(self, search_term, page_n)\u001b[0m\n\u001b[1;32m     58\u001b[0m sleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWAIT_FOR_ELEMENT_TIMEOUT)\n\u001b[1;32m     60\u001b[0m job_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job_card \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_all_elements_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob-card-list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_listing\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     62\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_job_card(job_card)\n\u001b[1;32m     63\u001b[0m     job_results\u001b[38;5;241m.\u001b[39mappend(job)\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/objects.py:93\u001b[0m, in \u001b[0;36mScraper.wait_for_all_elements_to_load\u001b[0;34m(self, by, name, base)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_all_elements_to_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mCLASS_NAME, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpv-top-card\u001b[39m\u001b[38;5;124m\"\u001b[39m, base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m     base \u001b[38;5;241m=\u001b[39m base \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT_FOR_ELEMENT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/support/wait.py:95\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_PAGES = 22\n",
    "SEARCH_KEYWORD = \"data\"\n",
    "\n",
    "jobs = []\n",
    "for page_n in range(1, N_PAGES+1):\n",
    "    pprint(f\"Searching jobs... Keyword: {SEARCH_KEYWORD}; Page {page_n}/{N_PAGES}\")\n",
    "    new_batch = job_search.search(SEARCH_KEYWORD, page_n)\n",
    "\n",
    "    # Check if the new batch of jobs are duplicates, \n",
    "    # which means we have gone through all the pages and should quit scraping.\n",
    "    if jobs and are_same(new_batch[0], jobs[0]):\n",
    "        pprint(\"Found duplicate results! All the pages have been scraped. Quiting...\")\n",
    "        break\n",
    "        \n",
    "    jobs.extend(new_batch)\n",
    "    pprint(f\"FINISHED PAGE: {page_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ca956d-a4cb-443d-8d9d-7bbc96e6e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "500ab9ca-d346-4b90-abb0-fafdffefca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl temporarily\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "with open(f\"../data/{fname}\", \"wb\") as f:\n",
    "    dicted_jobs = [job.to_dict() for job in jobs]\n",
    "    pickle.dump(dicted_jobs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f02c2-cd94-4e16-bb3e-3c3c4b761913",
   "metadata": {},
   "source": [
    "## 2. Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17c1166-5c4d-4ee4-a684-10c0535f32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job, actions\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "        self.job_title = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-unified-top-card__job-title')]\").text.strip()\n",
    "        self.company = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a[1]\").text.strip()\n",
    "        self.company_linkedin_url = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a\").get_attribute(\"href\")\n",
    "        self.location = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//*\").text.strip()\n",
    "        self.posted_date = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//span[3]\").text.strip()\n",
    "        self.job_type_1 = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')]/span\").text.strip()\n",
    "        self.job_description = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\").text.strip()\n",
    "        \n",
    "        try:\n",
    "            self.required_skills = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][1]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.required_skills += self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][2]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.job_type_2 = self.wait_for_element_to_load(by=By.XPATH, name=\"(//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')])[2]/span\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.job_type_2 = \"\"\n",
    "            \n",
    "        try:\n",
    "            self.applicant_count = self.wait_for_element_to_load(by=By.XPATH, name=\"jobs-unified-top-card__applicant-count\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.applicant_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.benefits = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'salary-main-rail-card')]\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.benefits = \"\"\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0690bf7-4c55-434f-a72b-fae89fd21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b010a69b-bf8d-434e-ba07-641b09ff211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up low-level servies for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5236-f815-4a94-b6b4-f0b368132f1e",
   "metadata": {},
   "source": [
    "Ignore the error logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b8e6e6-ed7c-4f70-8f48-8e5b463c681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../data/helsinki_data_jobs_2023-10-22.pkl\", \"rb\") as f:\n",
    "    jobs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c10b00c9-d1d6-4723-89a1-7823fd0f4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 160/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55fd0c4c74e3 <unknown>\n",
      "#1 0x55fd0c1f6c76 <unknown>\n",
      "#2 0x55fd0c232c96 <unknown>\n",
      "#3 0x55fd0c232dc1 <unknown>\n",
      "#4 0x55fd0c26c7f4 <unknown>\n",
      "#5 0x55fd0c25203d <unknown>\n",
      "#6 0x55fd0c26a30e <unknown>\n",
      "#7 0x55fd0c251de3 <unknown>\n",
      "#8 0x55fd0c2272dd <unknown>\n",
      "#9 0x55fd0c22834e <unknown>\n",
      "#10 0x55fd0c4873e4 <unknown>\n",
      "#11 0x55fd0c48b3d7 <unknown>\n",
      "#12 0x55fd0c495b20 <unknown>\n",
      "#13 0x55fd0c48c023 <unknown>\n",
      "#14 0x55fd0c45a1aa <unknown>\n",
      "#15 0x55fd0c4b06b8 <unknown>\n",
      "#16 0x55fd0c4b0847 <unknown>\n",
      "#17 0x55fd0c4c0243 <unknown>\n",
      "#18 0x7fd44c094ac3 <unknown>\n",
      "\n",
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55fd0c4c74e3 <unknown>\n",
      "#1 0x55fd0c1f6c76 <unknown>\n",
      "#2 0x55fd0c232c96 <unknown>\n",
      "#3 0x55fd0c232dc1 <unknown>\n",
      "#4 0x55fd0c26c7f4 <unknown>\n",
      "#5 0x55fd0c25203d <unknown>\n",
      "#6 0x55fd0c26a30e <unknown>\n",
      "#7 0x55fd0c251de3 <unknown>\n",
      "#8 0x55fd0c2272dd <unknown>\n",
      "#9 0x55fd0c22834e <unknown>\n",
      "#10 0x55fd0c4873e4 <unknown>\n",
      "#11 0x55fd0c48b3d7 <unknown>\n",
      "#12 0x55fd0c495b20 <unknown>\n",
      "#13 0x55fd0c48c023 <unknown>\n",
      "#14 0x55fd0c45a1aa <unknown>\n",
      "#15 0x55fd0c4b06b8 <unknown>\n",
      "#16 0x55fd0c4b0847 <unknown>\n",
      "#17 0x55fd0c4c0243 <unknown>\n",
      "#18 0x7fd44c094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: stale element not found\n  (Session info: headless chrome=114.0.5735.90); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n#0 0x55fd0c4c74e3 <unknown>\n#1 0x55fd0c1f6c76 <unknown>\n#2 0x55fd0c1fb53c <unknown>\n#3 0x55fd0c1fc97e <unknown>\n#4 0x55fd0c1fca4c <unknown>\n#5 0x55fd0c22d922 <unknown>\n#6 0x55fd0c252012 <unknown>\n#7 0x55fd0c228b03 <unknown>\n#8 0x55fd0c2521de <unknown>\n#9 0x55fd0c26a30e <unknown>\n#10 0x55fd0c251de3 <unknown>\n#11 0x55fd0c2272dd <unknown>\n#12 0x55fd0c22834e <unknown>\n#13 0x55fd0c4873e4 <unknown>\n#14 0x55fd0c48b3d7 <unknown>\n#15 0x55fd0c495b20 <unknown>\n#16 0x55fd0c48c023 <unknown>\n#17 0x55fd0c45a1aa <unknown>\n#18 0x55fd0c4b06b8 <unknown>\n#19 0x55fd0c4b0847 <unknown>\n#20 0x55fd0c4c0243 <unknown>\n#21 0x7fd44c094ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36m_Job.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:40\u001b[0m, in \u001b[0;36mJob.__init__\u001b[0;34m(self, linkedin_url, job_title, company, company_linkedin_url, location, posted_date, applicant_count, job_description, benefits, driver, close_on_complete, scrape)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m benefits\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scrape:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:47\u001b[0m, in \u001b[0;36mJob.scrape\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed_in():\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_logged_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis part is not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36m_Job.scrape_logged_in\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplicant_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_element_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//*[contains(@class, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msalary-main-rail-card\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:89\u001b[0m, in \u001b[0;36mWebElement.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The text of the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_ELEMENT_TEXT\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:394\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    393\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found\n  (Session info: headless chrome=114.0.5735.90); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n#0 0x55fd0c4c74e3 <unknown>\n#1 0x55fd0c1f6c76 <unknown>\n#2 0x55fd0c1fb53c <unknown>\n#3 0x55fd0c1fc97e <unknown>\n#4 0x55fd0c1fca4c <unknown>\n#5 0x55fd0c22d922 <unknown>\n#6 0x55fd0c252012 <unknown>\n#7 0x55fd0c228b03 <unknown>\n#8 0x55fd0c2521de <unknown>\n#9 0x55fd0c26a30e <unknown>\n#10 0x55fd0c251de3 <unknown>\n#11 0x55fd0c2272dd <unknown>\n#12 0x55fd0c22834e <unknown>\n#13 0x55fd0c4873e4 <unknown>\n#14 0x55fd0c48b3d7 <unknown>\n#15 0x55fd0c495b20 <unknown>\n#16 0x55fd0c48c023 <unknown>\n#17 0x55fd0c45a1aa <unknown>\n#18 0x55fd0c4b06b8 <unknown>\n#19 0x55fd0c4b0847 <unknown>\n#20 0x55fd0c4c0243 <unknown>\n#21 0x7fd44c094ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import sleep\n",
    "\n",
    "N_JOBS = len(jobs)\n",
    "\n",
    "crawled_jobs = []\n",
    "for i, job in enumerate(jobs):\n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "    crawled_jobs.append(_crawled_job)\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad0dbf8-cba2-42bc-a496-57345dde2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "260db594-e6f7-4ebc-9294-e132e9ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]\n",
    "                              ).drop(columns=[\"driver\"]\n",
    "                              ).drop_duplicates(\"linkedin_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9665-a746-4a98-a1f2-7446de35ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58dfc6-3ab7-416e-834f-525cb464aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crawled_jobs.drop_duplicates(\"linkedin_url\").to_csv(\"../data/crawled_jobs_1-236_checkpoint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568898a5-df54-4afa-85ff-3b858f901e30",
   "metadata": {},
   "source": [
    "### 2.1 Continue from the failed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50eff973-e5db-40c0-9a73-24e02e836e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 274/313\n",
      "Crawling... Jobs 275/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 276/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 277/313\n",
      "Crawling... Jobs 278/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 279/313\n",
      "Crawling... Jobs 280/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 281/313\n",
      "Crawling... Jobs 282/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 283/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 284/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 285/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 286/313\n",
      "Crawling... Jobs 287/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 288/313\n",
      "Crawling... Jobs 289/313\n",
      "Crawling... Jobs 290/313\n",
      "Crawling... Jobs 291/313\n",
      "Crawling... Jobs 292/313\n",
      "Crawling... Jobs 293/313\n",
      "Crawling... Jobs 294/313\n",
      "Crawling... Jobs 295/313\n",
      "Crawling... Jobs 296/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 297/313\n",
      "Crawling... Jobs 298/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 299/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 300/313\n",
      "Crawling... Jobs 301/313\n",
      "Crawling... Jobs 302/313\n",
      "Crawling... Jobs 303/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 304/313\n",
      "Crawling... Jobs 305/313\n",
      "Crawling... Jobs 306/313\n",
      "Crawling... Jobs 307/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 308/313\n",
      "Crawling... Jobs 309/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 310/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 311/313\n",
      "Crawling... Jobs 312/313\n",
      "Crawling... Jobs 313/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x563f4a26e4e3 <unknown>\n",
      "#1 0x563f49f9dc76 <unknown>\n",
      "#2 0x563f49fd9c96 <unknown>\n",
      "#3 0x563f49fd9dc1 <unknown>\n",
      "#4 0x563f4a0137f4 <unknown>\n",
      "#5 0x563f49ff903d <unknown>\n",
      "#6 0x563f4a01130e <unknown>\n",
      "#7 0x563f49ff8de3 <unknown>\n",
      "#8 0x563f49fce2dd <unknown>\n",
      "#9 0x563f49fcf34e <unknown>\n",
      "#10 0x563f4a22e3e4 <unknown>\n",
      "#11 0x563f4a2323d7 <unknown>\n",
      "#12 0x563f4a23cb20 <unknown>\n",
      "#13 0x563f4a233023 <unknown>\n",
      "#14 0x563f4a2011aa <unknown>\n",
      "#15 0x563f4a2576b8 <unknown>\n",
      "#16 0x563f4a257847 <unknown>\n",
      "#17 0x563f4a267243 <unknown>\n",
      "#18 0x7f40a4e94ac3 <unknown>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "\n",
    "CONTINUE_FROM = 236\n",
    "\n",
    "for i, job in enumerate(jobs):\n",
    "    if i+1<CONTINUE_FROM:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "    crawled_jobs.append(_crawled_job)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3595eb-d170-4e5a-b7d3-d86b8480a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RWE Scientist / Epidemiologist</td>\n",
       "      <td>Customer Relationship Management (CRM), Epidem...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737909849/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  1 week...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Data Engineering, Git, Python (Programming Lan...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3736532279/...</td>\n",
       "      <td>Suomen Palloliitto - Football Association of F...</td>\n",
       "      <td>https://www.linkedin.com/company/football-asso...</td>\n",
       "      <td>Suomen Palloliitto - Football Association of F...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nDATA ENGINEER\\n\\nSUOMEN PALLOLI...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Game Analyst</td>\n",
       "      <td>Analytical Skills, Data Analysis, Economics, M...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3717037977/...</td>\n",
       "      <td>Next Games, A Netflix Game Studio</td>\n",
       "      <td>https://www.linkedin.com/company/next-games/life</td>\n",
       "      <td>Next Games, A Netflix Game Studio · Helsinki, ...</td>\n",
       "      <td>Reposted  6 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nNext Games is a Netflix Game St...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Analysis, Data Science, Machine Learning,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3735986015/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  1 week...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science - Machine Learning Engineer</td>\n",
       "      <td>Artificial Intelligence (AI), Computer Science...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3629670334/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland Reposted  2 ...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nTeam purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Remote Work (Finnish Speakers) - Internet Ads ...</td>\n",
       "      <td>English and Finnish</td>\n",
       "      <td>Remote</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3728766620/...</td>\n",
       "      <td>TELUS International AI Data Solutions</td>\n",
       "      <td>https://www.linkedin.com/company/telusinternat...</td>\n",
       "      <td>TELUS International AI Data Solutions · Finlan...</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOur Company \\n\\nTELUS Internati...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Work From Home - Finnish Speakers (Internet Ad...</td>\n",
       "      <td>English and Finnish</td>\n",
       "      <td>Remote</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3731394329/...</td>\n",
       "      <td>TELUS International AI Data Solutions</td>\n",
       "      <td>https://www.linkedin.com/company/telusinternat...</td>\n",
       "      <td>TELUS International AI Data Solutions · Finlan...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nOur Company \\n\\nTELUS Internati...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Senior Engineering Manager (Bangkok based, rel...</td>\n",
       "      <td>Software DevelopmentC#, Engineering Management...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3616699365/...</td>\n",
       "      <td>Agoda</td>\n",
       "      <td>https://www.linkedin.com/company/agoda/life</td>\n",
       "      <td>Agoda · Helsinki, Uusimaa, Finland Reposted  3...</td>\n",
       "      <td>Reposted  3 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAbout Agoda\\n\\nAgoda is an onli...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Commercial Director Nordics, Transport Reagents</td>\n",
       "      <td>SwedishBusiness Planning, Commodities, Indirec...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3736132257/...</td>\n",
       "      <td>Yara Suomi</td>\n",
       "      <td>https://www.linkedin.com/company/yarasuomi/life</td>\n",
       "      <td>Yara Suomi · Espoo, Uusimaa, Finland  1 week a...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAbout Yara Industrial Solutions...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Sr. Sales Manager</td>\n",
       "      <td>Account Management, Business Planning, Busines...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3433869418/...</td>\n",
       "      <td>Supermicro</td>\n",
       "      <td>https://www.linkedin.com/company/supermicro/life</td>\n",
       "      <td>Supermicro · Helsinki, Uusimaa, Finland Repost...</td>\n",
       "      <td>Reposted  4 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Req ID: 22029\\n\\nAbout Supe...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  \\\n",
       "0                       RWE Scientist / Epidemiologist   \n",
       "1                                        Data Engineer   \n",
       "2                                  Senior Game Analyst   \n",
       "3                                       Data Scientist   \n",
       "4             Data Science - Machine Learning Engineer   \n",
       "..                                                 ...   \n",
       "307  Remote Work (Finnish Speakers) - Internet Ads ...   \n",
       "308  Work From Home - Finnish Speakers (Internet Ad...   \n",
       "309  Senior Engineering Manager (Bangkok based, rel...   \n",
       "310    Commercial Director Nordics, Transport Reagents   \n",
       "311                                  Sr. Sales Manager   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    Customer Relationship Management (CRM), Epidem...     Hybrid  Full-time   \n",
       "1    Data Engineering, Git, Python (Programming Lan...     Hybrid  Full-time   \n",
       "2    Analytical Skills, Data Analysis, Economics, M...     Hybrid  Full-time   \n",
       "3    Data Analysis, Data Science, Machine Learning,...     Hybrid  Full-time   \n",
       "4    Artificial Intelligence (AI), Computer Science...     Remote  Full-time   \n",
       "..                                                 ...        ...        ...   \n",
       "307                                English and Finnish     Remote              \n",
       "308                                English and Finnish     Remote              \n",
       "309  Software DevelopmentC#, Engineering Management...     Hybrid  Full-time   \n",
       "310  SwedishBusiness Planning, Commodities, Indirec...    On-site  Full-time   \n",
       "311  Account Management, Business Planning, Busines...  Full-time              \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3737909849/...   \n",
       "1    https://www.linkedin.com/jobs/view/3736532279/...   \n",
       "2    https://www.linkedin.com/jobs/view/3717037977/...   \n",
       "3    https://www.linkedin.com/jobs/view/3735986015/...   \n",
       "4    https://www.linkedin.com/jobs/view/3629670334/...   \n",
       "..                                                 ...   \n",
       "307  https://www.linkedin.com/jobs/view/3728766620/...   \n",
       "308  https://www.linkedin.com/jobs/view/3731394329/...   \n",
       "309  https://www.linkedin.com/jobs/view/3616699365/...   \n",
       "310  https://www.linkedin.com/jobs/view/3736132257/...   \n",
       "311  https://www.linkedin.com/jobs/view/3433869418/...   \n",
       "\n",
       "                                               company  \\\n",
       "0                                            MedEngine   \n",
       "1    Suomen Palloliitto - Football Association of F...   \n",
       "2                    Next Games, A Netflix Game Studio   \n",
       "3                                            MedEngine   \n",
       "4                                                 Wolt   \n",
       "..                                                 ...   \n",
       "307              TELUS International AI Data Solutions   \n",
       "308              TELUS International AI Data Solutions   \n",
       "309                                              Agoda   \n",
       "310                                         Yara Suomi   \n",
       "311                                         Supermicro   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0      https://www.linkedin.com/company/medengine/life   \n",
       "1    https://www.linkedin.com/company/football-asso...   \n",
       "2     https://www.linkedin.com/company/next-games/life   \n",
       "3      https://www.linkedin.com/company/medengine/life   \n",
       "4        https://www.linkedin.com/company/wolt-oy/life   \n",
       "..                                                 ...   \n",
       "307  https://www.linkedin.com/company/telusinternat...   \n",
       "308  https://www.linkedin.com/company/telusinternat...   \n",
       "309        https://www.linkedin.com/company/agoda/life   \n",
       "310    https://www.linkedin.com/company/yarasuomi/life   \n",
       "311   https://www.linkedin.com/company/supermicro/life   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    MedEngine · Helsinki, Uusimaa, Finland  1 week...             1 week ago   \n",
       "1    Suomen Palloliitto - Football Association of F...             1 week ago   \n",
       "2    Next Games, A Netflix Game Studio · Helsinki, ...   Reposted  6 days ago   \n",
       "3    MedEngine · Helsinki, Uusimaa, Finland  1 week...             1 week ago   \n",
       "4    Wolt · Helsinki, Uusimaa, Finland Reposted  2 ...  Reposted  2 weeks ago   \n",
       "..                                                 ...                    ...   \n",
       "307  TELUS International AI Data Solutions · Finlan...            3 weeks ago   \n",
       "308  TELUS International AI Data Solutions · Finlan...            2 weeks ago   \n",
       "309  Agoda · Helsinki, Uusimaa, Finland Reposted  3...   Reposted  3 days ago   \n",
       "310  Yara Suomi · Espoo, Uusimaa, Finland  1 week a...             1 week ago   \n",
       "311  Supermicro · Helsinki, Uusimaa, Finland Repost...   Reposted  4 days ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "1                  0  About the job\\nDATA ENGINEER\\n\\nSUOMEN PALLOLI...   \n",
       "2                  0  About the job\\nNext Games is a Netflix Game St...   \n",
       "3                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "4                  0  About the job\\nJob Description\\n\\nTeam purpose...   \n",
       "..               ...                                                ...   \n",
       "307                0  About the job\\nOur Company \\n\\nTELUS Internati...   \n",
       "308                0  About the job\\nOur Company \\n\\nTELUS Internati...   \n",
       "309                0  About the job\\nAbout Agoda\\n\\nAgoda is an onli...   \n",
       "310                0  About the job\\nAbout Yara Industrial Solutions...   \n",
       "311                0  About the job\\nJob Req ID: 22029\\n\\nAbout Supe...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "307           \n",
       "308           \n",
       "309           \n",
       "310           \n",
       "311           \n",
       "\n",
       "[312 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]).drop(columns=[\"driver\"]).drop_duplicates(\"linkedin_url\")\n",
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bf2bda0-7b3c-425d-87a4-0420c1e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"../data/crawled_jobs_n=312_{current_date}.csv\"\n",
    "\n",
    "# df_crawled_jobs.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0d8b-2cb2-4217-ab99-6ece5a995d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a1edd-7adb-4bbf-a672-fbfb84d14e32",
   "metadata": {},
   "source": [
    "# Scrape Linkedin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d7fc9e-da88-4ef3-9af4-85ef9ecd6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper==2.11.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have installed the dependency\n",
    "! pip freeze | grep linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50695a9e-6b21-44db-825b-922a213bbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Chrome 114.0.5735.90 \n"
     ]
    }
   ],
   "source": [
    "! google-chrome-stable --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f755f7ef-09a8-4caa-809f-551ba6949eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, Job, actions\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "\n",
    "class _JobSearch(JobSearch):\n",
    "    def __init__(self, final_url=None, **kwargs):\n",
    "        self.final_url = final_url\n",
    "        self.current_url = None\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def search(self, search_term: str, page_n) -> List[Job]:\n",
    "        if self.final_url is None:\n",
    "            self.current_url = os.path.join(self.base_url, \"search\") + f\"?keywords={urllib.parse.quote(search_term)}&refresh=true\"\n",
    "            self.driver.get(self.current_url)\n",
    "\n",
    "            # Get redirection URL\n",
    "            self.final_url = self.driver.current_url\n",
    "        else:\n",
    "            self.current_url = os.path.join(self.final_url, f\"&start={25*(page_n-1)}\")\n",
    "            self.driver.get(self.current_url)\n",
    "        \n",
    "        self.scroll_to_bottom()\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_listing_class_name = \"jobs-search-results-list\"\n",
    "        job_listing = self.wait_for_element_to_load(name=job_listing_class_name)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.3)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.6)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 1)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_results = []\n",
    "        for job_card in self.wait_for_all_elements_to_load(name=\"job-card-list\", base=job_listing):\n",
    "            job = self.scrape_job_card(job_card)\n",
    "            job_results.append(job)\n",
    "        return job_results\n",
    "\n",
    "def are_same(job1: Job, job2: Job):\n",
    "    if job1.job_title == job2.job_title and job1.company == job2.company:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612fd0-f5c4-4b3a-b2ee-578b132f471c",
   "metadata": {},
   "source": [
    "## 1. Scrape Job Search\n",
    "\n",
    "Scrape the first 20 pages of the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771e66e2-2364-4cdb-b4f1-f2abb4ec7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up the lower-level services for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\")\n",
    "job_search = _JobSearch(driver=driver, close_on_complete=False, scrape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d87d07e-659d-4abb-a058-e6d2c14e108e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Searching jobs... Keyword: data; Page 1/50'\n",
      "'FINISHED PAGE: 1'\n",
      "'Searching jobs... Keyword: data; Page 2/50'\n",
      "'FINISHED PAGE: 2'\n",
      "'Searching jobs... Keyword: data; Page 3/50'\n",
      "'FINISHED PAGE: 3'\n",
      "'Searching jobs... Keyword: data; Page 4/50'\n",
      "'FINISHED PAGE: 4'\n",
      "'Searching jobs... Keyword: data; Page 5/50'\n",
      "'FINISHED PAGE: 5'\n",
      "'Searching jobs... Keyword: data; Page 6/50'\n",
      "'FINISHED PAGE: 6'\n",
      "'Searching jobs... Keyword: data; Page 7/50'\n",
      "'FINISHED PAGE: 7'\n",
      "'Searching jobs... Keyword: data; Page 8/50'\n",
      "'FINISHED PAGE: 8'\n",
      "'Searching jobs... Keyword: data; Page 9/50'\n",
      "'FINISHED PAGE: 9'\n",
      "'Searching jobs... Keyword: data; Page 10/50'\n",
      "'FINISHED PAGE: 10'\n",
      "'Searching jobs... Keyword: data; Page 11/50'\n",
      "'FINISHED PAGE: 11'\n",
      "'Searching jobs... Keyword: data; Page 12/50'\n",
      "'FINISHED PAGE: 12'\n",
      "'Searching jobs... Keyword: data; Page 13/50'\n",
      "'FINISHED PAGE: 13'\n",
      "'Searching jobs... Keyword: data; Page 14/50'\n",
      "'FINISHED PAGE: 14'\n",
      "'Searching jobs... Keyword: data; Page 15/50'\n",
      "'FINISHED PAGE: 15'\n",
      "'Searching jobs... Keyword: data; Page 16/50'\n",
      "'FINISHED PAGE: 16'\n",
      "'Searching jobs... Keyword: data; Page 17/50'\n",
      "'FINISHED PAGE: 17'\n",
      "'Searching jobs... Keyword: data; Page 18/50'\n",
      "'FINISHED PAGE: 18'\n",
      "'Searching jobs... Keyword: data; Page 19/50'\n",
      "'FINISHED PAGE: 19'\n",
      "'Searching jobs... Keyword: data; Page 20/50'\n",
      "'FINISHED PAGE: 20'\n",
      "'Searching jobs... Keyword: data; Page 21/50'\n",
      "'FINISHED PAGE: 21'\n",
      "'Searching jobs... Keyword: data; Page 22/50'\n",
      "'SKIPPED PAGE: 22'\n",
      "'Searching jobs... Keyword: data; Page 23/50'\n",
      "'SKIPPED PAGE: 23'\n",
      "'Searching jobs... Keyword: data; Page 24/50'\n",
      "'SKIPPED PAGE: 24'\n",
      "'Searching jobs... Keyword: data; Page 25/50'\n",
      "'SKIPPED PAGE: 25'\n",
      "'Searching jobs... Keyword: data; Page 26/50'\n",
      "'SKIPPED PAGE: 26'\n",
      "'Searching jobs... Keyword: data; Page 27/50'\n",
      "'SKIPPED PAGE: 27'\n",
      "'Searching jobs... Keyword: data; Page 28/50'\n",
      "'SKIPPED PAGE: 28'\n",
      "'Searching jobs... Keyword: data; Page 29/50'\n",
      "'SKIPPED PAGE: 29'\n",
      "'Searching jobs... Keyword: data; Page 30/50'\n",
      "'SKIPPED PAGE: 30'\n",
      "'Searching jobs... Keyword: data; Page 31/50'\n",
      "'SKIPPED PAGE: 31'\n",
      "'Searching jobs... Keyword: data; Page 32/50'\n",
      "'SKIPPED PAGE: 32'\n",
      "'Searching jobs... Keyword: data; Page 33/50'\n",
      "'SKIPPED PAGE: 33'\n",
      "'Searching jobs... Keyword: data; Page 34/50'\n",
      "'SKIPPED PAGE: 34'\n",
      "'Searching jobs... Keyword: data; Page 35/50'\n",
      "'SKIPPED PAGE: 35'\n",
      "'Searching jobs... Keyword: data; Page 36/50'\n",
      "'SKIPPED PAGE: 36'\n",
      "'Searching jobs... Keyword: data; Page 37/50'\n",
      "'SKIPPED PAGE: 37'\n",
      "'Searching jobs... Keyword: data; Page 38/50'\n",
      "'SKIPPED PAGE: 38'\n",
      "'Searching jobs... Keyword: data; Page 39/50'\n",
      "'SKIPPED PAGE: 39'\n",
      "'Searching jobs... Keyword: data; Page 40/50'\n",
      "'SKIPPED PAGE: 40'\n",
      "'Searching jobs... Keyword: data; Page 41/50'\n",
      "'SKIPPED PAGE: 41'\n",
      "'Searching jobs... Keyword: data; Page 42/50'\n",
      "'FINISHED PAGE: 42'\n",
      "'Searching jobs... Keyword: data; Page 43/50'\n",
      "'FINISHED PAGE: 43'\n",
      "'Searching jobs... Keyword: data; Page 44/50'\n",
      "'FINISHED PAGE: 44'\n",
      "'Searching jobs... Keyword: data; Page 45/50'\n",
      "'FINISHED PAGE: 45'\n",
      "'Searching jobs... Keyword: data; Page 46/50'\n",
      "'FINISHED PAGE: 46'\n",
      "'Searching jobs... Keyword: data; Page 47/50'\n",
      "'FINISHED PAGE: 47'\n",
      "'Searching jobs... Keyword: data; Page 48/50'\n",
      "'FINISHED PAGE: 48'\n",
      "'Searching jobs... Keyword: data; Page 49/50'\n",
      "'FINISHED PAGE: 49'\n",
      "'Searching jobs... Keyword: data; Page 50/50'\n",
      "'FINISHED PAGE: 50'\n",
      "CPU times: user 4.34 s, sys: 569 ms, total: 4.91 s\n",
      "Wall time: 26min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "N_PAGES = 100\n",
    "SEARCH_KEYWORD = \"data\"\n",
    "\n",
    "jobs = []\n",
    "for page_n in range(1, N_PAGES+1):\n",
    "    pprint(f\"Searching jobs... Keyword: {SEARCH_KEYWORD}; Page {page_n}/{N_PAGES}\")\n",
    "    try:\n",
    "        new_batch = job_search.search(SEARCH_KEYWORD, page_n)\n",
    "    except TimeoutException:\n",
    "        pprint(f\"SKIPPED PAGE: {page_n}\")\n",
    "        continue\n",
    "\n",
    "    # Check if the new batch of jobs are duplicates, \n",
    "    # which means we have gone through all the pages and should quit scraping.\n",
    "    if jobs and are_same(new_batch[0], jobs[0]):\n",
    "        pprint(\"Found duplicate results! All the pages have been scraped. Quiting...\")\n",
    "        break\n",
    "        \n",
    "    jobs.extend(new_batch)\n",
    "    pprint(f\"FINISHED PAGE: {page_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ca956d-a4cb-443d-8d9d-7bbc96e6e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500ab9ca-d346-4b90-abb0-fafdffefca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl temporarily\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "with open(f\"../data/tmp/{fname}\", \"wb\") as f:\n",
    "    dicted_jobs = [job.to_dict() for job in jobs]\n",
    "    pickle.dump(dicted_jobs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f02c2-cd94-4e16-bb3e-3c3c4b761913",
   "metadata": {},
   "source": [
    "## 2. Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b17c1166-5c4d-4ee4-a684-10c0535f32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job, actions\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "        self.job_title = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-unified-top-card__job-title')]\").text.strip()\n",
    "        self.company = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a[1]\").text.strip()\n",
    "        self.company_linkedin_url = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a\").get_attribute(\"href\")\n",
    "        self.location = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//*\").text.strip()\n",
    "        self.posted_date = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//span[3]\").text.strip()\n",
    "        self.job_type_1 = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')]/span\").text.strip()\n",
    "        self.job_description = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\").text.strip()\n",
    "        \n",
    "        try:\n",
    "            self.required_skills = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][1]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.required_skills += self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][2]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.job_type_2 = self.wait_for_element_to_load(by=By.XPATH, name=\"(//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')])[2]/span\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.job_type_2 = \"\"\n",
    "            \n",
    "        try:\n",
    "            self.applicant_count = self.wait_for_element_to_load(by=By.XPATH, name=\"jobs-unified-top-card__applicant-count\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.applicant_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.benefits = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'salary-main-rail-card')]\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.benefits = \"\"\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0690bf7-4c55-434f-a72b-fae89fd21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b010a69b-bf8d-434e-ba07-641b09ff211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up low-level servies for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5236-f815-4a94-b6b4-f0b368132f1e",
   "metadata": {},
   "source": [
    "Ignore the error logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b8e6e6-ed7c-4f70-8f48-8e5b463c681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "\n",
    "with open(f\"../data/tmp/{fname}\", \"rb\") as f:\n",
    "    jobs = pickle.load(f)\n",
    "\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c10b00c9-d1d6-4723-89a1-7823fd0f4c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 5/384\n",
      "Crawling... Jobs 6/384\n",
      "Crawling... Jobs 7/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 8/384\n",
      "Crawling... Jobs 9/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 10/384\n",
      "Crawling... Jobs 11/384\n",
      "Crawling... Jobs 12/384\n",
      "Crawling... Jobs 13/384\n",
      "Crawling... Jobs 14/384\n",
      "Crawling... Jobs 15/384\n",
      "Crawling... Jobs 16/384\n",
      "Crawling... Jobs 17/384\n",
      "Crawling... Jobs 18/384\n",
      "Crawling... Jobs 19/384\n",
      "Crawling... Jobs 20/384\n",
      "Crawling... Jobs 21/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 22/384\n",
      "Crawling... Jobs 23/384\n",
      "Crawling... Jobs 24/384\n",
      "Crawling... Jobs 25/384\n",
      "Crawling... Jobs 26/384\n",
      "Crawling... Jobs 27/384\n",
      "Crawling... Jobs 28/384\n",
      "Crawling... Jobs 29/384\n",
      "Crawling... Jobs 30/384\n",
      "Crawling... Jobs 31/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 32/384\n",
      "Crawling... Jobs 33/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 34/384\n",
      "Crawling... Jobs 35/384\n",
      "Crawling... Jobs 36/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 37/384\n",
      "Crawling... Jobs 38/384\n",
      "Crawling... Jobs 39/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 40/384\n",
      "Crawling... Jobs 41/384\n",
      "Crawling... Jobs 42/384\n",
      "Crawling... Jobs 43/384\n",
      "Crawling... Jobs 44/384\n",
      "Crawling... Jobs 45/384\n",
      "Crawling... Jobs 46/384\n",
      "Crawling... Jobs 47/384\n",
      "Crawling... Jobs 48/384\n",
      "Crawling... Jobs 49/384\n",
      "Crawling... Jobs 50/384\n",
      "Crawling... Jobs 51/384\n",
      "Crawling... Jobs 52/384\n",
      "Crawling... Jobs 53/384\n",
      "Crawling... Jobs 54/384\n",
      "Crawling... Jobs 55/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 56/384\n",
      "Crawling... Jobs 57/384\n",
      "Crawling... Jobs 58/384\n",
      "Crawling... Jobs 59/384\n",
      "Crawling... Jobs 60/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 61/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 62/384\n",
      "Crawling... Jobs 63/384\n",
      "Crawling... Jobs 64/384\n",
      "Crawling... Jobs 65/384\n",
      "Crawling... Jobs 66/384\n",
      "Crawling... Jobs 67/384\n",
      "Crawling... Jobs 68/384\n",
      "Crawling... Jobs 69/384\n",
      "Crawling... Jobs 70/384\n",
      "Crawling... Jobs 71/384\n",
      "Crawling... Jobs 72/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 73/384\n",
      "Crawling... Jobs 74/384\n",
      "Crawling... Jobs 75/384\n",
      "Crawling... Jobs 76/384\n",
      "Crawling... Jobs 77/384\n",
      "Crawling... Jobs 78/384\n",
      "Crawling... Jobs 79/384\n",
      "Crawling... Jobs 80/384\n",
      "Crawling... Jobs 81/384\n",
      "Crawling... Jobs 82/384\n",
      "Crawling... Jobs 83/384\n",
      "Crawling... Jobs 84/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 85/384\n",
      "Crawling... Jobs 86/384\n",
      "Crawling... Jobs 87/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 88/384\n",
      "Crawling... Jobs 89/384\n",
      "Crawling... Jobs 90/384\n",
      "Crawling... Jobs 91/384\n",
      "Crawling... Jobs 92/384\n",
      "Crawling... Jobs 93/384\n",
      "Crawling... Jobs 94/384\n",
      "Crawling... Jobs 95/384\n",
      "Crawling... Jobs 96/384\n",
      "Crawling... Jobs 97/384\n",
      "Crawling... Jobs 98/384\n",
      "Crawling... Jobs 99/384\n",
      "Crawling... Jobs 100/384\n",
      "Crawling... Jobs 101/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 102/384\n",
      "Crawling... Jobs 103/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 104/384\n",
      "Crawling... Jobs 105/384\n",
      "Crawling... Jobs 106/384\n",
      "Crawling... Jobs 107/384\n",
      "Crawling... Jobs 108/384\n",
      "Crawling... Jobs 109/384\n",
      "Crawling... Jobs 110/384\n",
      "Crawling... Jobs 111/384\n",
      "Crawling... Jobs 112/384\n",
      "Crawling... Jobs 113/384\n",
      "Crawling... Jobs 114/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 115/384\n",
      "Crawling... Jobs 116/384\n",
      "Crawling... Jobs 117/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 118/384\n",
      "Crawling... Jobs 119/384\n",
      "Crawling... Jobs 120/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 121/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 122/384\n",
      "Crawling... Jobs 123/384\n",
      "Crawling... Jobs 124/384\n",
      "Crawling... Jobs 125/384\n",
      "Crawling... Jobs 126/384\n",
      "Crawling... Jobs 127/384\n",
      "Crawling... Jobs 128/384\n",
      "Crawling... Jobs 129/384\n",
      "Crawling... Jobs 130/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 131/384\n",
      "Crawling... Jobs 132/384\n",
      "Crawling... Jobs 133/384\n",
      "Crawling... Jobs 134/384\n",
      "Crawling... Jobs 135/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 136/384\n",
      "Crawling... Jobs 137/384\n",
      "Crawling... Jobs 138/384\n",
      "Crawling... Jobs 139/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 140/384\n",
      "Crawling... Jobs 141/384\n",
      "Crawling... Jobs 142/384\n",
      "Crawling... Jobs 143/384\n",
      "Crawling... Jobs 144/384\n",
      "Crawling... Jobs 145/384\n",
      "Crawling... Jobs 146/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x5608466024e3 <unknown>\n",
      "#1 0x560846331c76 <unknown>\n",
      "#2 0x56084636dc96 <unknown>\n",
      "#3 0x56084636ddc1 <unknown>\n",
      "#4 0x5608463a77f4 <unknown>\n",
      "#5 0x56084638d03d <unknown>\n",
      "#6 0x5608463a530e <unknown>\n",
      "#7 0x56084638cde3 <unknown>\n",
      "#8 0x5608463622dd <unknown>\n",
      "#9 0x56084636334e <unknown>\n",
      "#10 0x5608465c23e4 <unknown>\n",
      "#11 0x5608465c63d7 <unknown>\n",
      "#12 0x5608465d0b20 <unknown>\n",
      "#13 0x5608465c7023 <unknown>\n",
      "#14 0x5608465951aa <unknown>\n",
      "#15 0x5608465eb6b8 <unknown>\n",
      "#16 0x5608465eb847 <unknown>\n",
      "#17 0x5608465fb243 <unknown>\n",
      "#18 0x7fd7bbc94ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 147/384\n",
      "Crawling... Jobs 148/384\n",
      "Crawling... Jobs 149/384\n",
      "Crawling... Jobs 150/384\n",
      "Crawling... Jobs 151/384\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x5608466024e3 <unknown>\n#1 0x560846331c76 <unknown>\n#2 0x560846329c7f <unknown>\n#3 0x56084631bca2 <unknown>\n#4 0x56084631d412 <unknown>\n#5 0x56084631c0ca <unknown>\n#6 0x56084631b168 <unknown>\n#7 0x56084631afa0 <unknown>\n#8 0x5608463199bf <unknown>\n#9 0x560846319fed <unknown>\n#10 0x560846333b06 <unknown>\n#11 0x5608463a59e5 <unknown>\n#12 0x56084638d012 <unknown>\n#13 0x5608463a530e <unknown>\n#14 0x56084638cde3 <unknown>\n#15 0x5608463622dd <unknown>\n#16 0x56084636334e <unknown>\n#17 0x5608465c23e4 <unknown>\n#18 0x5608465c63d7 <unknown>\n#19 0x5608465d0b20 <unknown>\n#20 0x5608465c7023 <unknown>\n#21 0x5608465951aa <unknown>\n#22 0x5608465eb6b8 <unknown>\n#23 0x5608465eb847 <unknown>\n#24 0x5608465fb243 <unknown>\n#25 0x7fd7bbc94ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36m_Job.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:40\u001b[0m, in \u001b[0;36mJob.__init__\u001b[0;34m(self, linkedin_url, job_title, company, company_linkedin_url, location, posted_date, applicant_count, job_description, benefits, driver, close_on_complete, scrape)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenefits \u001b[38;5;241m=\u001b[39m benefits\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scrape:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/linkedin_scraper/jobs.py:47\u001b[0m, in \u001b[0;36mJob.scrape\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed_in():\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_logged_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_on_complete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_on_complete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis part is not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36m_Job.scrape_logged_in\u001b[0;34m(self, close_on_complete)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_logged_in\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_on_complete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinkedin_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfocus()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_element_to_load(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mXPATH, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//*[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobs-unified-top-card__job-title\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repos/ds-employment-landscape/.venv/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n  (Session info: headless chrome=114.0.5735.90)\nStacktrace:\n#0 0x5608466024e3 <unknown>\n#1 0x560846331c76 <unknown>\n#2 0x560846329c7f <unknown>\n#3 0x56084631bca2 <unknown>\n#4 0x56084631d412 <unknown>\n#5 0x56084631c0ca <unknown>\n#6 0x56084631b168 <unknown>\n#7 0x56084631afa0 <unknown>\n#8 0x5608463199bf <unknown>\n#9 0x560846319fed <unknown>\n#10 0x560846333b06 <unknown>\n#11 0x5608463a59e5 <unknown>\n#12 0x56084638d012 <unknown>\n#13 0x5608463a530e <unknown>\n#14 0x56084638cde3 <unknown>\n#15 0x5608463622dd <unknown>\n#16 0x56084636334e <unknown>\n#17 0x5608465c23e4 <unknown>\n#18 0x5608465c63d7 <unknown>\n#19 0x5608465d0b20 <unknown>\n#20 0x5608465c7023 <unknown>\n#21 0x5608465951aa <unknown>\n#22 0x5608465eb6b8 <unknown>\n#23 0x5608465eb847 <unknown>\n#24 0x5608465fb243 <unknown>\n#25 0x7fd7bbc94ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from time import sleep\n",
    "\n",
    "N_JOBS = len(jobs)\n",
    "\n",
    "crawled_jobs = []\n",
    "for i, job in enumerate(jobs):\n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except StaleElementReferenceException:\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad0dbf8-cba2-42bc-a496-57345dde2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "260db594-e6f7-4ebc-9294-e132e9ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]\n",
    "                              ).drop(columns=[\"driver\"]\n",
    "                              ).drop_duplicates(\"linkedin_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc2d9665-a746-4a98-a1f2-7446de35ac9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Information Security Manager / Datasäkerhetschef</td>\n",
       "      <td>English, Finnish, and SwedishBusiness Administ...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3746294030/...</td>\n",
       "      <td>Hanken School of Economics</td>\n",
       "      <td>https://www.linkedin.com/company/hanken-svensk...</td>\n",
       "      <td>Hanken School of Economics · Helsinki, Uusimaa...</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nIT-Services at Hanken School of...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qlik Sense Administrator</td>\n",
       "      <td>Analytical Skills and Cloud ComputingCross-fun...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3744707657/...</td>\n",
       "      <td>Sievo</td>\n",
       "      <td>https://www.linkedin.com/company/sievo-oy/life</td>\n",
       "      <td>Sievo · Helsinki, Uusimaa, Finland  1 week ago...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAre you our next Qlik Sense / Q...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Revenue Analyst</td>\n",
       "      <td>Campaign Analytics, Problem Solving, Revenue M...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3707060022/...</td>\n",
       "      <td>Warner Bros. Discovery</td>\n",
       "      <td>https://www.linkedin.com/company/warner-bros-d...</td>\n",
       "      <td>Warner Bros. Discovery · Finland Reposted  2 w...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nEvery great story has a new beg...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Analysis, Data Science, Machine Learning,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3735986015/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  2 week...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Data Analytics, Data Engineering, Data Science...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3750477070/...</td>\n",
       "      <td>The Hub</td>\n",
       "      <td>https://www.linkedin.com/company/thehubio/life</td>\n",
       "      <td>The Hub · Helsinki, Uusimaa, Finland  17 hours...</td>\n",
       "      <td>17 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAbout Huuva\\n\\nHuuva Kitchens t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Staff Security Engineer</td>\n",
       "      <td>Communication, Data Privacy, Ethical Hacking, ...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3711096063/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland Reposted  1 ...</td>\n",
       "      <td>Reposted  1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nWolt is look...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Senior Backend Engineer, Shelf Product Recogni...</td>\n",
       "      <td>Back-End Web Development, Databases, and Pytho...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3615509468/...</td>\n",
       "      <td>Scandit</td>\n",
       "      <td>https://www.linkedin.com/company/scandit/life</td>\n",
       "      <td>Scandit · Finland Reposted  2 weeks ago  · Ove...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\n**This role could also be based...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(Senior) Data Architect - Tietoevry Tech Servi...</td>\n",
       "      <td>Big Data, Data Integration, and Data Warehousi...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737890407/...</td>\n",
       "      <td>Tietoevry</td>\n",
       "      <td>https://www.linkedin.com/company/tietoevry/life</td>\n",
       "      <td>Tietoevry · Tampere, Pirkanmaa, Finland  2 wee...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nYou may apply to Tietoevry by s...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(Senior) Data Architect - Tietoevry Tech Servi...</td>\n",
       "      <td>Big Data, Data Integration, and Data Warehousi...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3737889673/...</td>\n",
       "      <td>Tietoevry</td>\n",
       "      <td>https://www.linkedin.com/company/tietoevry/life</td>\n",
       "      <td>Tietoevry · Turku, Southwest Finland, Finland ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nYou may apply to Tietoevry by s...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Supply Chain Software Developer - 46186BR</td>\n",
       "      <td>Computer Science, Databases, and Software Deve...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3727889480/...</td>\n",
       "      <td>Synopsys Inc</td>\n",
       "      <td>https://www.linkedin.com/company/synopsys/life</td>\n",
       "      <td>Synopsys Inc · Finland Reposted  2 weeks ago  ...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAs a Supply Chain Software Deve...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  \\\n",
       "0     Information Security Manager / Datasäkerhetschef   \n",
       "1                             Qlik Sense Administrator   \n",
       "2                                      Revenue Analyst   \n",
       "3                                       Data Scientist   \n",
       "4                                        Data Engineer   \n",
       "..                                                 ...   \n",
       "145                            Staff Security Engineer   \n",
       "146  Senior Backend Engineer, Shelf Product Recogni...   \n",
       "147  (Senior) Data Architect - Tietoevry Tech Servi...   \n",
       "148  (Senior) Data Architect - Tietoevry Tech Servi...   \n",
       "149          Supply Chain Software Developer - 46186BR   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    English, Finnish, and SwedishBusiness Administ...    On-site  Full-time   \n",
       "1    Analytical Skills and Cloud ComputingCross-fun...     Hybrid  Full-time   \n",
       "2    Campaign Analytics, Problem Solving, Revenue M...    On-site  Full-time   \n",
       "3    Data Analysis, Data Science, Machine Learning,...     Hybrid  Full-time   \n",
       "4    Data Analytics, Data Engineering, Data Science...     Hybrid  Full-time   \n",
       "..                                                 ...        ...        ...   \n",
       "145  Communication, Data Privacy, Ethical Hacking, ...     Remote  Full-time   \n",
       "146  Back-End Web Development, Databases, and Pytho...     Hybrid  Full-time   \n",
       "147  Big Data, Data Integration, and Data Warehousi...     Hybrid  Full-time   \n",
       "148  Big Data, Data Integration, and Data Warehousi...     Hybrid  Full-time   \n",
       "149  Computer Science, Databases, and Software Deve...  Full-time              \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3746294030/...   \n",
       "1    https://www.linkedin.com/jobs/view/3744707657/...   \n",
       "2    https://www.linkedin.com/jobs/view/3707060022/...   \n",
       "3    https://www.linkedin.com/jobs/view/3735986015/...   \n",
       "4    https://www.linkedin.com/jobs/view/3750477070/...   \n",
       "..                                                 ...   \n",
       "145  https://www.linkedin.com/jobs/view/3711096063/...   \n",
       "146  https://www.linkedin.com/jobs/view/3615509468/...   \n",
       "147  https://www.linkedin.com/jobs/view/3737890407/...   \n",
       "148  https://www.linkedin.com/jobs/view/3737889673/...   \n",
       "149  https://www.linkedin.com/jobs/view/3727889480/...   \n",
       "\n",
       "                        company  \\\n",
       "0    Hanken School of Economics   \n",
       "1                         Sievo   \n",
       "2        Warner Bros. Discovery   \n",
       "3                     MedEngine   \n",
       "4                       The Hub   \n",
       "..                          ...   \n",
       "145                        Wolt   \n",
       "146                     Scandit   \n",
       "147                   Tietoevry   \n",
       "148                   Tietoevry   \n",
       "149                Synopsys Inc   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0    https://www.linkedin.com/company/hanken-svensk...   \n",
       "1       https://www.linkedin.com/company/sievo-oy/life   \n",
       "2    https://www.linkedin.com/company/warner-bros-d...   \n",
       "3      https://www.linkedin.com/company/medengine/life   \n",
       "4       https://www.linkedin.com/company/thehubio/life   \n",
       "..                                                 ...   \n",
       "145      https://www.linkedin.com/company/wolt-oy/life   \n",
       "146      https://www.linkedin.com/company/scandit/life   \n",
       "147    https://www.linkedin.com/company/tietoevry/life   \n",
       "148    https://www.linkedin.com/company/tietoevry/life   \n",
       "149     https://www.linkedin.com/company/synopsys/life   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    Hanken School of Economics · Helsinki, Uusimaa...             2 days ago   \n",
       "1    Sievo · Helsinki, Uusimaa, Finland  1 week ago...             1 week ago   \n",
       "2    Warner Bros. Discovery · Finland Reposted  2 w...  Reposted  2 weeks ago   \n",
       "3    MedEngine · Helsinki, Uusimaa, Finland  2 week...            2 weeks ago   \n",
       "4    The Hub · Helsinki, Uusimaa, Finland  17 hours...           17 hours ago   \n",
       "..                                                 ...                    ...   \n",
       "145  Wolt · Helsinki, Uusimaa, Finland Reposted  1 ...   Reposted  1 week ago   \n",
       "146  Scandit · Finland Reposted  2 weeks ago  · Ove...  Reposted  2 weeks ago   \n",
       "147  Tietoevry · Tampere, Pirkanmaa, Finland  2 wee...            2 weeks ago   \n",
       "148  Tietoevry · Turku, Southwest Finland, Finland ...            2 weeks ago   \n",
       "149  Synopsys Inc · Finland Reposted  2 weeks ago  ...  Reposted  2 weeks ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nIT-Services at Hanken School of...   \n",
       "1                  0  About the job\\nAre you our next Qlik Sense / Q...   \n",
       "2                  0  About the job\\nEvery great story has a new beg...   \n",
       "3                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "4                  0  About the job\\nAbout Huuva\\n\\nHuuva Kitchens t...   \n",
       "..               ...                                                ...   \n",
       "145                0  About the job\\nJob Description\\n\\nWolt is look...   \n",
       "146                0  About the job\\n**This role could also be based...   \n",
       "147                0  About the job\\nYou may apply to Tietoevry by s...   \n",
       "148                0  About the job\\nYou may apply to Tietoevry by s...   \n",
       "149                0  About the job\\nAs a Supply Chain Software Deve...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "145           \n",
       "146           \n",
       "147           \n",
       "148           \n",
       "149           \n",
       "\n",
       "[150 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b58dfc6-3ab7-416e-834f-525cb464aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crawled_jobs.to_csv(f\"../data/crawled_jobs_1-{len(crawled_jobs}_checkpoint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568898a5-df54-4afa-85ff-3b858f901e30",
   "metadata": {},
   "source": [
    "### 2.1 Continue from the failed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50eff973-e5db-40c0-9a73-24e02e836e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 360/384\n",
      "Crawling... Jobs 361/384\n",
      "Crawling... Jobs 362/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55ae0b27f4e3 <unknown>\n",
      "#1 0x55ae0afaec76 <unknown>\n",
      "#2 0x55ae0afeac96 <unknown>\n",
      "#3 0x55ae0afeadc1 <unknown>\n",
      "#4 0x55ae0b0247f4 <unknown>\n",
      "#5 0x55ae0b00a03d <unknown>\n",
      "#6 0x55ae0b02230e <unknown>\n",
      "#7 0x55ae0b009de3 <unknown>\n",
      "#8 0x55ae0afdf2dd <unknown>\n",
      "#9 0x55ae0afe034e <unknown>\n",
      "#10 0x55ae0b23f3e4 <unknown>\n",
      "#11 0x55ae0b2433d7 <unknown>\n",
      "#12 0x55ae0b24db20 <unknown>\n",
      "#13 0x55ae0b244023 <unknown>\n",
      "#14 0x55ae0b2121aa <unknown>\n",
      "#15 0x55ae0b2686b8 <unknown>\n",
      "#16 0x55ae0b268847 <unknown>\n",
      "#17 0x55ae0b278243 <unknown>\n",
      "#18 0x7f22a1094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 363/384\n",
      "Crawling... Jobs 364/384\n",
      "Crawling... Jobs 365/384\n",
      "Crawling... Jobs 366/384\n",
      "Crawling... Jobs 367/384\n",
      "Crawling... Jobs 368/384\n",
      "Crawling... Jobs 369/384\n",
      "Crawling... Jobs 370/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55ae0b27f4e3 <unknown>\n",
      "#1 0x55ae0afaec76 <unknown>\n",
      "#2 0x55ae0afeac96 <unknown>\n",
      "#3 0x55ae0afeadc1 <unknown>\n",
      "#4 0x55ae0b0247f4 <unknown>\n",
      "#5 0x55ae0b00a03d <unknown>\n",
      "#6 0x55ae0b02230e <unknown>\n",
      "#7 0x55ae0b009de3 <unknown>\n",
      "#8 0x55ae0afdf2dd <unknown>\n",
      "#9 0x55ae0afe034e <unknown>\n",
      "#10 0x55ae0b23f3e4 <unknown>\n",
      "#11 0x55ae0b2433d7 <unknown>\n",
      "#12 0x55ae0b24db20 <unknown>\n",
      "#13 0x55ae0b244023 <unknown>\n",
      "#14 0x55ae0b2121aa <unknown>\n",
      "#15 0x55ae0b2686b8 <unknown>\n",
      "#16 0x55ae0b268847 <unknown>\n",
      "#17 0x55ae0b278243 <unknown>\n",
      "#18 0x7f22a1094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 371/384\n",
      "Crawling... Jobs 372/384\n",
      "Crawling... Jobs 373/384\n",
      "Crawling... Jobs 374/384\n",
      "Crawling... Jobs 375/384\n",
      "Crawling... Jobs 376/384\n",
      "Crawling... Jobs 377/384\n",
      "Crawling... Jobs 378/384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Message: \n",
      "Stacktrace:\n",
      "#0 0x55ae0b27f4e3 <unknown>\n",
      "#1 0x55ae0afaec76 <unknown>\n",
      "#2 0x55ae0afeac96 <unknown>\n",
      "#3 0x55ae0afeadc1 <unknown>\n",
      "#4 0x55ae0b0247f4 <unknown>\n",
      "#5 0x55ae0b00a03d <unknown>\n",
      "#6 0x55ae0b02230e <unknown>\n",
      "#7 0x55ae0b009de3 <unknown>\n",
      "#8 0x55ae0afdf2dd <unknown>\n",
      "#9 0x55ae0afe034e <unknown>\n",
      "#10 0x55ae0b23f3e4 <unknown>\n",
      "#11 0x55ae0b2433d7 <unknown>\n",
      "#12 0x55ae0b24db20 <unknown>\n",
      "#13 0x55ae0b244023 <unknown>\n",
      "#14 0x55ae0b2121aa <unknown>\n",
      "#15 0x55ae0b2686b8 <unknown>\n",
      "#16 0x55ae0b268847 <unknown>\n",
      "#17 0x55ae0b278243 <unknown>\n",
      "#18 0x7f22a1094ac3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 379/384\n",
      "Crawling... Jobs 380/384\n",
      "Crawling... Jobs 381/384\n",
      "Crawling... Jobs 382/384\n",
      "Crawling... Jobs 383/384\n",
      "Crawling... Jobs 384/384\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "\n",
    "CONTINUE_FROM = 360\n",
    "\n",
    "for i, job in enumerate(jobs):\n",
    "    if i+1<CONTINUE_FROM:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "    crawled_jobs.append(_crawled_job)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3595eb-d170-4e5a-b7d3-d86b8480a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>job_type_1</th>\n",
       "      <th>job_type_2</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_url</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>applicant_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Information Security Manager / Datasäkerhetschef</td>\n",
       "      <td>English, Finnish, and SwedishBusiness Administ...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3746294030/...</td>\n",
       "      <td>Hanken School of Economics</td>\n",
       "      <td>https://www.linkedin.com/company/hanken-svensk...</td>\n",
       "      <td>Hanken School of Economics · Helsinki, Uusimaa...</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nIT-Services at Hanken School of...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qlik Sense Administrator</td>\n",
       "      <td>Analytical Skills and Cloud ComputingCross-fun...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3744707657/...</td>\n",
       "      <td>Sievo</td>\n",
       "      <td>https://www.linkedin.com/company/sievo-oy/life</td>\n",
       "      <td>Sievo · Helsinki, Uusimaa, Finland  1 week ago...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAre you our next Qlik Sense / Q...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Revenue Analyst</td>\n",
       "      <td>Campaign Analytics, Problem Solving, Revenue M...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3707060022/...</td>\n",
       "      <td>Warner Bros. Discovery</td>\n",
       "      <td>https://www.linkedin.com/company/warner-bros-d...</td>\n",
       "      <td>Warner Bros. Discovery · Finland Reposted  2 w...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nEvery great story has a new beg...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Analysis, Data Science, Machine Learning,...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3735986015/...</td>\n",
       "      <td>MedEngine</td>\n",
       "      <td>https://www.linkedin.com/company/medengine/life</td>\n",
       "      <td>MedEngine · Helsinki, Uusimaa, Finland  2 week...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nMedEngine is a digitally minded...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Data Analytics, Data Engineering, Data Science...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3750477070/...</td>\n",
       "      <td>The Hub</td>\n",
       "      <td>https://www.linkedin.com/company/thehubio/life</td>\n",
       "      <td>The Hub · Helsinki, Uusimaa, Finland  17 hours...</td>\n",
       "      <td>17 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAbout Huuva\\n\\nHuuva Kitchens t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Data Engineering, Data Warehousing, and Extrac...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3741252798/...</td>\n",
       "      <td>Trimble Inc.</td>\n",
       "      <td>https://www.linkedin.com/company/trimble/life</td>\n",
       "      <td>Trimble Inc. · Espoo, Uusimaa, Finland Reposte...</td>\n",
       "      <td>Reposted  1 week ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nYour Title: Data Engineer\\n\\nJo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Databricks data engineer</td>\n",
       "      <td>Data Analytics, Data Engineering, Data Science...</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3729429877/...</td>\n",
       "      <td>Accenture Nordics</td>\n",
       "      <td>https://www.linkedin.com/company/accenture-nor...</td>\n",
       "      <td>Accenture Nordics · Helsinki, Uusimaa, Finland...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nHaemme Databricks data engineer...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Expert IT Developer to join Quality Research a...</td>\n",
       "      <td>Databases, Programming, Python (Programming La...</td>\n",
       "      <td>On-site</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3733082364/...</td>\n",
       "      <td>Nordea</td>\n",
       "      <td>https://www.linkedin.com/company/nordea/life</td>\n",
       "      <td>Nordea · Helsinki, Uusimaa, Finland  2 weeks a...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob ID: 19431\\n We are now look...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Machine Learning Engineer - MLOps</td>\n",
       "      <td>Artificial Intelligence (AI), Data Mining, Dat...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3750358338/...</td>\n",
       "      <td>Wolt</td>\n",
       "      <td>https://www.linkedin.com/company/wolt-oy/life</td>\n",
       "      <td>Wolt · Helsinki, Uusimaa, Finland  1 day ago  ...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nJob Description\\n\\nTeam purpose...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Senior Threat Detection Engineer - 5G Cybersec...</td>\n",
       "      <td>Computer Science and CybersecurityCommunicatio...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3719551297/...</td>\n",
       "      <td>Ericsson</td>\n",
       "      <td>https://www.linkedin.com/company/ericsson/life</td>\n",
       "      <td>Ericsson · Jorvas, Uusimaa, Finland Reposted  ...</td>\n",
       "      <td>Reposted  2 weeks ago</td>\n",
       "      <td>0</td>\n",
       "      <td>About the job\\nAbout This Opportunity\\n\\nJoin ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  \\\n",
       "0     Information Security Manager / Datasäkerhetschef   \n",
       "1                             Qlik Sense Administrator   \n",
       "2                                      Revenue Analyst   \n",
       "3                                       Data Scientist   \n",
       "4                                        Data Engineer   \n",
       "..                                                 ...   \n",
       "378                                      Data Engineer   \n",
       "379                           Databricks data engineer   \n",
       "380  Expert IT Developer to join Quality Research a...   \n",
       "381                  Machine Learning Engineer - MLOps   \n",
       "382  Senior Threat Detection Engineer - 5G Cybersec...   \n",
       "\n",
       "                                       required_skills job_type_1 job_type_2  \\\n",
       "0    English, Finnish, and SwedishBusiness Administ...    On-site  Full-time   \n",
       "1    Analytical Skills and Cloud ComputingCross-fun...     Hybrid  Full-time   \n",
       "2    Campaign Analytics, Problem Solving, Revenue M...    On-site  Full-time   \n",
       "3    Data Analysis, Data Science, Machine Learning,...     Hybrid  Full-time   \n",
       "4    Data Analytics, Data Engineering, Data Science...     Hybrid  Full-time   \n",
       "..                                                 ...        ...        ...   \n",
       "378  Data Engineering, Data Warehousing, and Extrac...     Remote  Full-time   \n",
       "379  Data Analytics, Data Engineering, Data Science...     Hybrid  Full-time   \n",
       "380  Databases, Programming, Python (Programming La...    On-site  Full-time   \n",
       "381  Artificial Intelligence (AI), Data Mining, Dat...     Remote  Full-time   \n",
       "382  Computer Science and CybersecurityCommunicatio...  Full-time              \n",
       "\n",
       "                                          linkedin_url  \\\n",
       "0    https://www.linkedin.com/jobs/view/3746294030/...   \n",
       "1    https://www.linkedin.com/jobs/view/3744707657/...   \n",
       "2    https://www.linkedin.com/jobs/view/3707060022/...   \n",
       "3    https://www.linkedin.com/jobs/view/3735986015/...   \n",
       "4    https://www.linkedin.com/jobs/view/3750477070/...   \n",
       "..                                                 ...   \n",
       "378  https://www.linkedin.com/jobs/view/3741252798/...   \n",
       "379  https://www.linkedin.com/jobs/view/3729429877/...   \n",
       "380  https://www.linkedin.com/jobs/view/3733082364/...   \n",
       "381  https://www.linkedin.com/jobs/view/3750358338/...   \n",
       "382  https://www.linkedin.com/jobs/view/3719551297/...   \n",
       "\n",
       "                        company  \\\n",
       "0    Hanken School of Economics   \n",
       "1                         Sievo   \n",
       "2        Warner Bros. Discovery   \n",
       "3                     MedEngine   \n",
       "4                       The Hub   \n",
       "..                          ...   \n",
       "378                Trimble Inc.   \n",
       "379           Accenture Nordics   \n",
       "380                      Nordea   \n",
       "381                        Wolt   \n",
       "382                    Ericsson   \n",
       "\n",
       "                                  company_linkedin_url  \\\n",
       "0    https://www.linkedin.com/company/hanken-svensk...   \n",
       "1       https://www.linkedin.com/company/sievo-oy/life   \n",
       "2    https://www.linkedin.com/company/warner-bros-d...   \n",
       "3      https://www.linkedin.com/company/medengine/life   \n",
       "4       https://www.linkedin.com/company/thehubio/life   \n",
       "..                                                 ...   \n",
       "378      https://www.linkedin.com/company/trimble/life   \n",
       "379  https://www.linkedin.com/company/accenture-nor...   \n",
       "380       https://www.linkedin.com/company/nordea/life   \n",
       "381      https://www.linkedin.com/company/wolt-oy/life   \n",
       "382     https://www.linkedin.com/company/ericsson/life   \n",
       "\n",
       "                                              location            posted_date  \\\n",
       "0    Hanken School of Economics · Helsinki, Uusimaa...             2 days ago   \n",
       "1    Sievo · Helsinki, Uusimaa, Finland  1 week ago...             1 week ago   \n",
       "2    Warner Bros. Discovery · Finland Reposted  2 w...  Reposted  2 weeks ago   \n",
       "3    MedEngine · Helsinki, Uusimaa, Finland  2 week...            2 weeks ago   \n",
       "4    The Hub · Helsinki, Uusimaa, Finland  17 hours...           17 hours ago   \n",
       "..                                                 ...                    ...   \n",
       "378  Trimble Inc. · Espoo, Uusimaa, Finland Reposte...   Reposted  1 week ago   \n",
       "379  Accenture Nordics · Helsinki, Uusimaa, Finland...  Reposted  2 weeks ago   \n",
       "380  Nordea · Helsinki, Uusimaa, Finland  2 weeks a...            2 weeks ago   \n",
       "381  Wolt · Helsinki, Uusimaa, Finland  1 day ago  ...              1 day ago   \n",
       "382  Ericsson · Jorvas, Uusimaa, Finland Reposted  ...  Reposted  2 weeks ago   \n",
       "\n",
       "     applicant_count                                    job_description  \\\n",
       "0                  0  About the job\\nIT-Services at Hanken School of...   \n",
       "1                  0  About the job\\nAre you our next Qlik Sense / Q...   \n",
       "2                  0  About the job\\nEvery great story has a new beg...   \n",
       "3                  0  About the job\\nMedEngine is a digitally minded...   \n",
       "4                  0  About the job\\nAbout Huuva\\n\\nHuuva Kitchens t...   \n",
       "..               ...                                                ...   \n",
       "378                0  About the job\\nYour Title: Data Engineer\\n\\nJo...   \n",
       "379                0  About the job\\nHaemme Databricks data engineer...   \n",
       "380                0  About the job\\nJob ID: 19431\\n We are now look...   \n",
       "381                0  About the job\\nJob Description\\n\\nTeam purpose...   \n",
       "382                0  About the job\\nAbout This Opportunity\\n\\nJoin ...   \n",
       "\n",
       "    benefits  \n",
       "0             \n",
       "1             \n",
       "2             \n",
       "3             \n",
       "4             \n",
       "..       ...  \n",
       "378           \n",
       "379           \n",
       "380           \n",
       "381           \n",
       "382           \n",
       "\n",
       "[383 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]).drop(columns=[\"driver\"]).drop_duplicates(\"linkedin_url\")\n",
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2bda0-7b3c-425d-87a4-0420c1e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"../data/crawled_jobs_{current_date}.csv\"\n",
    "\n",
    "# df_crawled_jobs.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0d8b-2cb2-4217-ab99-6ece5a995d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

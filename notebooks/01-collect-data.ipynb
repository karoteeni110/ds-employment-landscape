{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a1edd-7adb-4bbf-a672-fbfb84d14e32",
   "metadata": {},
   "source": [
    "# Scrape Linkedin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d7fc9e-da88-4ef3-9af4-85ef9ecd6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin-scraper==2.11.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have installed the dependency\n",
    "! pip freeze | grep linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50695a9e-6b21-44db-825b-922a213bbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Chrome 114.0.5735.90 \n"
     ]
    }
   ],
   "source": [
    "! google-chrome-stable --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755f7ef-09a8-4caa-809f-551ba6949eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, Job, actions\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options\n",
    "\n",
    "class _JobSearch(JobSearch):\n",
    "    def __init__(self, final_url=None, **kwargs):\n",
    "        self.final_url = final_url\n",
    "        self.current_url = None\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def search(self, search_term: str, page_n) -> List[Job]:\n",
    "        if self.final_url is None:\n",
    "            self.current_url = os.path.join(self.base_url, \"search\") + f\"?keywords={urllib.parse.quote(search_term)}&refresh=true\"\n",
    "            self.driver.get(self.current_url)\n",
    "\n",
    "            # Get redirection URL\n",
    "            self.final_url = self.driver.current_url\n",
    "        else:\n",
    "            self.current_url = os.path.join(self.final_url, f\"&start={25*(page_n-1)}\")\n",
    "            self.driver.get(self.current_url)\n",
    "        \n",
    "        self.scroll_to_bottom()\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_listing_class_name = \"jobs-search-results-list\"\n",
    "        job_listing = self.wait_for_element_to_load(name=job_listing_class_name)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.3)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 0.6)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        self.scroll_class_name_element_to_page_percent(job_listing_class_name, 1)\n",
    "        self.focus()\n",
    "        sleep(self.WAIT_FOR_ELEMENT_TIMEOUT)\n",
    "\n",
    "        job_results = []\n",
    "        for job_card in self.wait_for_all_elements_to_load(name=\"job-card-list\", base=job_listing):\n",
    "            job = self.scrape_job_card(job_card)\n",
    "            job_results.append(job)\n",
    "        return job_results\n",
    "\n",
    "def are_same(job1: Job, job2: Job):\n",
    "    if job1.job_title == job2.job_title and job1.company == job2.company:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612fd0-f5c4-4b3a-b2ee-578b132f471c",
   "metadata": {},
   "source": [
    "## 1. Scrape Job Search\n",
    "\n",
    "Scrape the first 50 pages of the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771e66e2-2364-4cdb-b4f1-f2abb4ec7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up the lower-level services for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) # if email and password isnt given, it'll prompt in terminal\n",
    "print(\"... Logged in.\")\n",
    "job_search = _JobSearch(driver=driver, close_on_complete=False, scrape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d87d07e-659d-4abb-a058-e6d2c14e108e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Searching jobs... Keyword: data; Page 1/50'\n",
      "'FINISHED PAGE: 1'\n",
      "'Searching jobs... Keyword: data; Page 2/50'\n",
      "'FINISHED PAGE: 2'\n",
      "'Searching jobs... Keyword: data; Page 3/50'\n",
      "'FINISHED PAGE: 3'\n",
      "'Searching jobs... Keyword: data; Page 4/50'\n",
      "'FINISHED PAGE: 4'\n",
      "'Searching jobs... Keyword: data; Page 5/50'\n",
      "'FINISHED PAGE: 5'\n",
      "'Searching jobs... Keyword: data; Page 6/50'\n",
      "'FINISHED PAGE: 6'\n",
      "'Searching jobs... Keyword: data; Page 7/50'\n",
      "'FINISHED PAGE: 7'\n",
      "'Searching jobs... Keyword: data; Page 8/50'\n",
      "'FINISHED PAGE: 8'\n",
      "'Searching jobs... Keyword: data; Page 9/50'\n",
      "'FINISHED PAGE: 9'\n",
      "'Searching jobs... Keyword: data; Page 10/50'\n",
      "'FINISHED PAGE: 10'\n",
      "'Searching jobs... Keyword: data; Page 11/50'\n",
      "'FINISHED PAGE: 11'\n",
      "'Searching jobs... Keyword: data; Page 12/50'\n",
      "'FINISHED PAGE: 12'\n",
      "'Searching jobs... Keyword: data; Page 13/50'\n",
      "'FINISHED PAGE: 13'\n",
      "'Searching jobs... Keyword: data; Page 14/50'\n",
      "'FINISHED PAGE: 14'\n",
      "'Searching jobs... Keyword: data; Page 15/50'\n",
      "'FINISHED PAGE: 15'\n",
      "'Searching jobs... Keyword: data; Page 16/50'\n",
      "'FINISHED PAGE: 16'\n",
      "'Searching jobs... Keyword: data; Page 17/50'\n",
      "'FINISHED PAGE: 17'\n",
      "'Searching jobs... Keyword: data; Page 18/50'\n",
      "'FINISHED PAGE: 18'\n",
      "'Searching jobs... Keyword: data; Page 19/50'\n",
      "'FINISHED PAGE: 19'\n",
      "'Searching jobs... Keyword: data; Page 20/50'\n",
      "'FINISHED PAGE: 20'\n",
      "'Searching jobs... Keyword: data; Page 21/50'\n",
      "'FINISHED PAGE: 21'\n",
      "'Searching jobs... Keyword: data; Page 22/50'\n",
      "'FINISHED PAGE: 22'\n",
      "'Searching jobs... Keyword: data; Page 23/50'\n",
      "'SKIPPED PAGE: 23'\n",
      "'Searching jobs... Keyword: data; Page 24/50'\n",
      "'SKIPPED PAGE: 24'\n",
      "'Searching jobs... Keyword: data; Page 25/50'\n",
      "'SKIPPED PAGE: 25'\n",
      "'Searching jobs... Keyword: data; Page 26/50'\n",
      "'SKIPPED PAGE: 26'\n",
      "'Searching jobs... Keyword: data; Page 27/50'\n",
      "'SKIPPED PAGE: 27'\n",
      "'Searching jobs... Keyword: data; Page 28/50'\n",
      "'SKIPPED PAGE: 28'\n",
      "'Searching jobs... Keyword: data; Page 29/50'\n",
      "'SKIPPED PAGE: 29'\n",
      "'Searching jobs... Keyword: data; Page 30/50'\n",
      "'SKIPPED PAGE: 30'\n",
      "'Searching jobs... Keyword: data; Page 31/50'\n",
      "'SKIPPED PAGE: 31'\n",
      "'Searching jobs... Keyword: data; Page 32/50'\n",
      "'SKIPPED PAGE: 32'\n",
      "'Searching jobs... Keyword: data; Page 33/50'\n",
      "'SKIPPED PAGE: 33'\n",
      "'Searching jobs... Keyword: data; Page 34/50'\n",
      "'SKIPPED PAGE: 34'\n",
      "'Searching jobs... Keyword: data; Page 35/50'\n",
      "'SKIPPED PAGE: 35'\n",
      "'Searching jobs... Keyword: data; Page 36/50'\n",
      "'SKIPPED PAGE: 36'\n",
      "'Searching jobs... Keyword: data; Page 37/50'\n",
      "'SKIPPED PAGE: 37'\n",
      "'Searching jobs... Keyword: data; Page 38/50'\n",
      "'SKIPPED PAGE: 38'\n",
      "'Searching jobs... Keyword: data; Page 39/50'\n",
      "'SKIPPED PAGE: 39'\n",
      "'Searching jobs... Keyword: data; Page 40/50'\n",
      "'SKIPPED PAGE: 40'\n",
      "'Searching jobs... Keyword: data; Page 41/50'\n",
      "'SKIPPED PAGE: 41'\n",
      "'Searching jobs... Keyword: data; Page 42/50'\n",
      "'FINISHED PAGE: 42'\n",
      "'Searching jobs... Keyword: data; Page 43/50'\n",
      "'FINISHED PAGE: 43'\n",
      "'Searching jobs... Keyword: data; Page 44/50'\n",
      "'FINISHED PAGE: 44'\n",
      "'Searching jobs... Keyword: data; Page 45/50'\n",
      "'FINISHED PAGE: 45'\n",
      "'Searching jobs... Keyword: data; Page 46/50'\n",
      "'FINISHED PAGE: 46'\n",
      "'Searching jobs... Keyword: data; Page 47/50'\n",
      "'FINISHED PAGE: 47'\n",
      "'Searching jobs... Keyword: data; Page 48/50'\n",
      "'FINISHED PAGE: 48'\n",
      "'Searching jobs... Keyword: data; Page 49/50'\n",
      "'FINISHED PAGE: 49'\n",
      "'Searching jobs... Keyword: data; Page 50/50'\n",
      "'FINISHED PAGE: 50'\n",
      "CPU times: user 3.56 s, sys: 637 ms, total: 4.19 s\n",
      "Wall time: 23min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "N_PAGES = 50\n",
    "SEARCH_KEYWORD = \"data\"\n",
    "\n",
    "jobs = []\n",
    "for page_n in range(1, N_PAGES+1):\n",
    "    pprint(f\"Searching jobs... Keyword: {SEARCH_KEYWORD}; Page {page_n}/{N_PAGES}\")\n",
    "    try:\n",
    "        new_batch = job_search.search(SEARCH_KEYWORD, page_n)\n",
    "    except TimeoutException:\n",
    "        pprint(f\"SKIPPED PAGE: {page_n}\")\n",
    "        continue\n",
    "\n",
    "    # Check if the new batch of jobs are duplicates, \n",
    "    # which means we have gone through all the pages and should quit scraping.\n",
    "    if jobs and are_same(new_batch[0], jobs[0]):\n",
    "        pprint(\"Found duplicate results! All the pages have been scraped. Quiting...\")\n",
    "        break\n",
    "        \n",
    "    jobs.extend(new_batch)\n",
    "    pprint(f\"FINISHED PAGE: {page_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ca956d-a4cb-443d-8d9d-7bbc96e6e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500ab9ca-d346-4b90-abb0-fafdffefca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl temporarily\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "with open(f\"../data/tmp/{fname}\", \"wb\") as f:\n",
    "    dicted_jobs = [job.to_dict() for job in jobs]\n",
    "    pickle.dump(dicted_jobs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f02c2-cd94-4e16-bb3e-3c3c4b761913",
   "metadata": {},
   "source": [
    "## 2. Scrape job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17c1166-5c4d-4ee4-a684-10c0535f32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from linkedin_scraper import Job, actions\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class _Job(Job):\n",
    "    def __init__(self, **kwargs):\n",
    "       self.job_title = \"\"\n",
    "       self.required_skills = \"\"\n",
    "       self.job_type_1 = \"\"\n",
    "       self.job_type_2 = \"\"\n",
    " \n",
    "       super().__init__(**kwargs)\n",
    "    \n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        \n",
    "        driver.get(self.linkedin_url)\n",
    "        self.focus()\n",
    "        self.job_title = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-unified-top-card__job-title')]\").text.strip()\n",
    "        self.company = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a[1]\").text.strip()\n",
    "        self.company_linkedin_url = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//a\").get_attribute(\"href\")\n",
    "        self.location = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//*\").text.strip()\n",
    "        self.posted_date = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-jobs-unified-top-card__primary-description')]//span[3]\").text.strip()\n",
    "        self.job_type_1 = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')]/span\").text.strip()\n",
    "        self.job_description = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'jobs-description')]\").text.strip()\n",
    "        \n",
    "        try:\n",
    "            self.required_skills = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][1]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.required_skills += self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'job-details-how-you-match__skills-item')][2]//a\").text.strip()\n",
    "        except TimeoutException as e:\n",
    "            logger.error(str(e))\n",
    "\n",
    "        try:\n",
    "            self.job_type_2 = self.wait_for_element_to_load(by=By.XPATH, name=\"(//*[contains(@class, 'ui-label ui-label--accent-3 text-body-small')])[2]/span\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.job_type_2 = \"\"\n",
    "            \n",
    "        try:\n",
    "            self.applicant_count = self.wait_for_element_to_load(by=By.XPATH, name=\"jobs-unified-top-card__applicant-count\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.applicant_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.benefits = self.wait_for_element_to_load(by=By.XPATH, name=\"//*[contains(@class, 'salary-main-rail-card')]\").text.strip()\n",
    "        except TimeoutException:\n",
    "            self.benefits = \"\"\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0690bf7-4c55-434f-a72b-fae89fd21906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "def set_chrome_options() -> Options:\n",
    "    \"\"\"Sets chrome options for Selenium.\n",
    "    Chrome options for headless browser is enabled.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b010a69b-bf8d-434e-ba07-641b09ff211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logged in.\n"
     ]
    }
   ],
   "source": [
    "# Set up low-level servies for scraping\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5236-f815-4a94-b6b4-f0b368132f1e",
   "metadata": {},
   "source": [
    "Ignore the error logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b8e6e6-ed7c-4f70-8f48-8e5b463c681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"helsinki_data_jobs_{current_date}.pkl\"\n",
    "\n",
    "with open(f\"../data/tmp/{fname}\", \"rb\") as f:\n",
    "    jobs = pickle.load(f)\n",
    "\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c10b00c9-d1d6-4723-89a1-7823fd0f4c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... Jobs 335/392\n",
      "Crawling... Jobs 336/392\n",
      "Crawling... Jobs 337/392\n",
      "Crawling... Jobs 338/392\n",
      "Crawling... Jobs 339/392\n",
      "Crawling... Jobs 340/392\n",
      "Crawling... Jobs 341/392\n",
      "Crawling... Jobs 342/392\n",
      "Crawling... Jobs 343/392\n",
      "Crawling... Jobs 344/392\n",
      "Crawling... Jobs 345/392\n",
      "Crawling... Jobs 346/392\n",
      "Crawling... Jobs 347/392\n",
      "Crawling... Jobs 348/392\n",
      "Crawling... Jobs 349/392\n",
      "Crawling... Jobs 350/392\n",
      "Crawling... Jobs 351/392\n",
      "Crawling... Jobs 352/392\n",
      "Crawling... Jobs 353/392\n",
      "Crawling... Jobs 354/392\n",
      "Crawling... Jobs 355/392\n",
      "Crawling... Jobs 356/392\n",
      "Crawling... Jobs 357/392\n",
      "Crawling... Jobs 358/392\n",
      "Crawling... Jobs 359/392\n",
      "Crawling... Jobs 360/392\n",
      "Crawling... Jobs 361/392\n",
      "Crawling... Jobs 362/392\n",
      "Crawling... Jobs 363/392\n",
      "Crawling... Jobs 364/392\n",
      "Crawling... Jobs 365/392\n",
      "Crawling... Jobs 366/392\n",
      "Crawling... Jobs 367/392\n",
      "Crawling... Jobs 368/392\n",
      "Crawling... Jobs 369/392\n",
      "Crawling... Jobs 370/392\n",
      "Crawling... Jobs 371/392\n",
      "Crawling... Jobs 372/392\n",
      "Crawling... Jobs 373/392\n",
      "Crawling... Jobs 374/392\n",
      "Crawling... Jobs 375/392\n",
      "Crawling... Jobs 376/392\n",
      "Crawling... Jobs 377/392\n",
      "Crawling... Jobs 378/392\n",
      "Crawling... Jobs 379/392\n",
      "Crawling... Jobs 380/392\n",
      "Crawling... Jobs 381/392\n",
      "Crawling... Jobs 382/392\n",
      "Crawling... Jobs 383/392\n",
      "Crawling... Jobs 384/392\n",
      "Crawling... Jobs 385/392\n",
      "Crawling... Jobs 386/392\n",
      "Crawling... Jobs 387/392\n",
      "Crawling... Jobs 388/392\n",
      "Crawling... Jobs 389/392\n",
      "Crawling... Jobs 390/392\n",
      "Crawling... Jobs 391/392\n",
      "Crawling... Jobs 392/392\n",
      "CPU times: user 19 s, sys: 3.64 s, total: 22.6 s\n",
      "Wall time: 1h 35min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from time import sleep\n",
    "\n",
    "N_JOBS = len(jobs)\n",
    "\n",
    "crawled_jobs = []\n",
    "for i, job in enumerate(jobs):\n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except StaleElementReferenceException or TimeoutException:\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad0dbf8-cba2-42bc-a496-57345dde2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "260db594-e6f7-4ebc-9294-e132e9ae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]\n",
    "                              ).drop(columns=[\"driver\"]\n",
    "                              ).drop_duplicates(\"linkedin_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9665-a746-4a98-a1f2-7446de35ac9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58dfc6-3ab7-416e-834f-525cb464aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crawled_jobs.to_csv(f\"../data/crawled_jobs_1-{len(crawled_jobs}_checkpoint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568898a5-df54-4afa-85ff-3b858f901e30",
   "metadata": {},
   "source": [
    "### 2.1 Continue from the failed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352c75e-aaa9-4bd8-ac86-add24d96cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case session expiration\n",
    "driver = webdriver.Chrome(options=set_chrome_options())\n",
    "actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "print(\"... Logged in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eff973-e5db-40c0-9a73-24e02e836e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Continue\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, WebDriverException\n",
    "\n",
    "CONTINUE_FROM = 78\n",
    "\n",
    "for i, job in enumerate(jobs):\n",
    "    if i+1<CONTINUE_FROM:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Crawling... Jobs {i+1}/{N_JOBS}\")\n",
    "    try:\n",
    "        _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "        crawled_jobs.append(_crawled_job)\n",
    "        sleep(1)\n",
    "    except (StaleElementReferenceException, TimeoutException):\n",
    "        print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "        sleep(1)\n",
    "        continue\n",
    "    except (WebDriverException, TypeError):\n",
    "        print(\"Session expired. Logging in...\") \n",
    "        driver = webdriver.Chrome(options=set_chrome_options())\n",
    "        actions.login(driver, os.environ[\"EMAIL\"], os.environ[\"PWORD\"]) \n",
    "        print(\"... Logged in.\")\n",
    "        \n",
    "        try: #Redo crawling\n",
    "            _crawled_job = _Job(linkedin_url=job.get(\"linkedin_url\"), driver=driver, close_on_complete=False, scrape=True)\n",
    "            crawled_jobs.append(_crawled_job)\n",
    "        except:\n",
    "            print(f\"... Skipped Job {i+1}/{N_JOBS}.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3595eb-d170-4e5a-b7d3-d86b8480a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_crawled_jobs = pd.DataFrame([vars(job) for job in crawled_jobs]).drop(columns=[\"driver\"]).drop_duplicates(\"linkedin_url\")\n",
    "df_crawled_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2bda0-7b3c-425d-87a4-0420c1e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save today's crawl\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fname = f\"../data/crawled_jobs_{current_date}.csv\"\n",
    "\n",
    "df_crawled_jobs.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0d8b-2cb2-4217-ab99-6ece5a995d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94094-a7ba-4dd8-96d6-17b734384714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
